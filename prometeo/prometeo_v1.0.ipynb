{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROMETEO_v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paquetería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "import textwrap\n",
    "import tiktoken\n",
    "import umap.umap_ as umap\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Cuenta el número de tokens en el documento\n",
    "    proporcionado.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def reduce_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Esta función toma un conjunto de embeddings de\n",
    "    alta dimensión y los reduce a una dimensión menor (2).\"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "def get_optimal_clusters(embeddings: np.ndarray, max_clusters: int = 50, random_state: int = 1234):\n",
    "    \"\"\"Obtiene el número óptimo de clústers.\"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    bics = [GaussianMixture(n_components=n, random_state=random_state).fit(embeddings).bic(embeddings)\n",
    "            for n in range(1, max_clusters)]\n",
    "    return np.argmin(bics) + 1\n",
    "\n",
    "def gmm_clustering(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"Clusteriza con el método GMM.\"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state).fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "def format_cluster_texts(df):\n",
    "    \"\"\"Agrupa los textos de cada clúster en listas.\"\"\"\n",
    "    clustered_texts = {}\n",
    "    for cluster in df['Cluster'].unique():\n",
    "        cluster_texts = df[df['Cluster'] == cluster]['Texto'].tolist()\n",
    "        clustered_texts[cluster] = \" --- \".join(cluster_texts)\n",
    "    return clustered_texts\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=80):\n",
    "    \"\"\"Formato para respuestas.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    \"\"\"Generador de referencias.\"\"\"\n",
    "    print(wrap_text_preserve_newlines(llm_response['answer']))\n",
    "    print('\\nReferencias:')\n",
    "    for contexto in llm_response[\"context\"][:5]:\n",
    "        print(contexto)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "embeddings = OpenAIEmbeddings(\n",
    ")\n",
    "\n",
    "detailed_llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "template = \"\"\"Tu tarea es generar un resumen extremadamente detallado del siguiente\n",
    "texto: {text} \"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | detailed_llm | StrOutputParser()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.5,\n",
    "    model_name='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Quieres crear o cargar un tema de conversación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"¿Crear (1) o cargar (2) un tema de conversación?: \")\n",
    "\n",
    "if user_input.lower() == \"1\":\n",
    "    print(f'Elegiste crear de cero un tema de conversación.\\n')\n",
    "\n",
    "    # Asegúrate de que haya PDFs en la carpeta 'docs'\n",
    "    documents = DirectoryLoader('./docs/', glob=\"./*.pdf\", loader_cls=PyPDFLoader).load()\n",
    "    # Tratameinto de caracteres indeseados\n",
    "    for d in documents:\n",
    "        d.page_content = d.page_content.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    texts = [doc.page_content for doc in docs]\n",
    "\n",
    "    d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "    d_reversed = list(reversed(d_sorted))\n",
    "    concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "        [doc.page_content for doc in d_reversed]\n",
    "    )\n",
    "    print(\n",
    "        \"Número de tokens en el documento proporcionado: %s\"\n",
    "        % num_tokens_from_string(concatenated_content)\n",
    "    )\n",
    "\n",
    "    global_embeddings = [embeddings.embed_query(txt) for txt in texts]\n",
    "\n",
    "    topic_name = input('¿Cómo se llama el tema de conversación?')\n",
    "\n",
    "    embed_name = topic_name + '_emb' + '.txt'\n",
    "    with open(rf'./{embed_name}', 'w') as f:\n",
    "        for i in global_embeddings:\n",
    "            f.write(\"%s\\n\" % i)\n",
    "    print(f'\\nEstás usando el tema de conversación: {embed_name}')\n",
    "\n",
    "    dim = 2\n",
    "    global_embeddings_reduced = reduce_cluster_embeddings(global_embeddings, dim)\n",
    "    labels, _ = gmm_clustering(global_embeddings_reduced, threshold=0.5)\n",
    "    simple_labels = [label[0] if len(label) > 0 else -1 for label in labels]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Texto': texts,\n",
    "        'Embedding': list(global_embeddings_reduced),\n",
    "        'Cluster': simple_labels\n",
    "    })\n",
    "\n",
    "    clustered_texts = format_cluster_texts(df)\n",
    "    summaries = {}\n",
    "    for cluster, text in clustered_texts.items():\n",
    "        summary = chain.invoke({\"text\": text})\n",
    "        summaries[cluster] = summary\n",
    "    embedded_summaries = [embeddings.embed_query(summary) for summary in summaries.values()]\n",
    "    embedded_summaries_np = np.array(embedded_summaries)\n",
    "    labels, _ = gmm_clustering(embedded_summaries_np, threshold=0.5)\n",
    "    simple_labels = [label[0] if len(label) > 0 else -1 for label in labels]\n",
    "    clustered_summaries = {}\n",
    "    for i, label in enumerate(simple_labels):\n",
    "        if label not in clustered_summaries:\n",
    "            clustered_summaries[label] = []\n",
    "        clustered_summaries[label].append(list(summaries.values())[i])\n",
    "    final_summaries = {}\n",
    "    for cluster, texts in clustered_summaries.items():\n",
    "        combined_text = ' '.join(texts)\n",
    "        summary = chain.invoke({\"text\": combined_text})\n",
    "        final_summaries[cluster] = summary\n",
    "    texts_from_df = df['Texto'].tolist()\n",
    "    texts_from_clustered_texts = list(clustered_texts.values())\n",
    "    texts_from_final_summaries = list(final_summaries.values())\n",
    "\n",
    "    combined_texts = texts_from_df + texts_from_clustered_texts + texts_from_final_summaries\n",
    "\n",
    "    file_name = topic_name + '.txt'\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        for t in combined_texts:\n",
    "            f.write(\"%s\\n\" % t)\n",
    "\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    textos = text_splitter.split_text(content) #chunked_knowledge in v2\n",
    "\n",
    "    persist_directory = topic_name + '_kb'\n",
    "    vectorstore = Chroma.from_texts(texts=textos,\n",
    "                                    embedding=embeddings,\n",
    "                                    persist_directory=persist_directory)\n",
    "    vectorstore.persist()\n",
    "    vectorstore = None\n",
    "    os.system(f'zip -r db.zip ./{persist_directory}')\n",
    "\n",
    "    vectorstore = Chroma(persist_directory=persist_directory,\n",
    "                         embedding_function=embeddings)\n",
    "\n",
    "    def adjust_final_number(string: str, max_threshold: int, initial_number: int) -> int:\n",
    "        final_number = initial_number\n",
    "        while final_number < max_threshold:\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": final_number})\n",
    "            docs = retriever.invoke(string)\n",
    "            text = \"\".join([doc.page_content for doc in docs])\n",
    "            if num_tokens_from_string(text) < max_threshold:\n",
    "                final_number += 1\n",
    "            else:\n",
    "                break\n",
    "        return final_number\n",
    "\n",
    "    final_number = adjust_final_number(\"¿Cuál es el tema principal del documento?\", 10000, 4)\n",
    "    print(f'\\nK final es: {final_number}')\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": final_number})\n",
    "    \n",
    "elif user_input.lower() == \"2\":\n",
    "    print('Elegiste cargar un tema de conversación ya creado.\\n')\n",
    "    global_embeddings = []\n",
    "\n",
    "    topic_name = input('¿Cómo se llama el tema de conversación?')\n",
    "\n",
    "    embed_name = topic_name + '_emb' + '.txt'\n",
    "    print(f'Estás usando el tema de conversación: {embed_name}\\n')\n",
    "    with open(rf'./{embed_name}', 'r') as f:\n",
    "        for i in f:\n",
    "            x = ast.literal_eval(i.strip())  # Convertir la cadena a lista de números\n",
    "            global_embeddings.append(x)\n",
    "\n",
    "    global_embeddings = np.array(global_embeddings, dtype=float)\n",
    "\n",
    "    file_name = topic_name + '.txt'\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    textos = text_splitter.split_text(content)\n",
    "\n",
    "    persist_directory = topic_name + '_kb'\n",
    "    vectorstore = Chroma(persist_directory=persist_directory, \n",
    "                    embedding_function=embeddings)\n",
    "\n",
    "    def adjust_final_number(string: str, max_threshold: int, initial_number: int) -> int:\n",
    "        final_number = initial_number\n",
    "        while final_number < max_threshold:\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": final_number})\n",
    "            docs = retriever.invoke(string)\n",
    "            text = \"\".join([doc.page_content for doc in docs])\n",
    "            if num_tokens_from_string(text) < max_threshold:\n",
    "                final_number += 1\n",
    "            else:\n",
    "                break\n",
    "        return final_number\n",
    "\n",
    "    final_number = adjust_final_number(\"¿Cuál es el tema principal del documento?\", 10000, 4)\n",
    "    print(f'K final es: {final_number}')\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": final_number})\n",
    "    \n",
    "elif user_input != \"1\" and user_input != \"2\":\n",
    "    print('No seleccionaste ningún tema de conversación.\\n')\n",
    "\n",
    "# Se personaliza el LLM #\n",
    "\n",
    "template = \"\"\"\n",
    "Eres Prometeo, un asistente especializado en revisión bibliográfica, que habla Español.\n",
    "\n",
    "Tu tarea consiste en proporcionar respuestas extremadamente detalladas y basadas en evidencia a \n",
    "cualquier pregunta relacionada con el siguiente contexto, obtenido de un artículo científico: {context}.\n",
    "\n",
    "Tu respuesta debe centrarse en los aspectos más relevantes de la literatura científica sin mencionar \n",
    "directamente el contexto proporcionado. Identifica y utiliza palabras clave para enfocarte en los temas \n",
    "más importantes para dar una respuesta más precisa.\n",
    "\n",
    "Siempre responde en Español, excepto en nombres propios, y no menciones detalles sobre ti a menos que se \n",
    "te pregunte directamente.\n",
    "\n",
    "Finalmente y teniendo en cuento lo anterior, responde la siguiente pregunta: {question}\n",
    "\"\"\"\n",
    "\n",
    "selected_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | selected_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script para múltiples preguntas a la vez\n",
    "\n",
    "preguntas_lab = [\n",
    "'¿Cuál es el título del documento?',\n",
    "'¿Puedes generar una cita en formato APA del documento?',\n",
    "'¿Cuál es el tema principal del documento?',\n",
    "'Según el autor, ¿cómo se define el aprendizaje experiencial?',\n",
    "'Según el autor, ¿cuál o cuáles son las teorías que explican el aprendizaje experiencial',\n",
    "'Según el autor, ¿cómo se puede aplicar el aprendizaje experiencial en contextos educativos?',\n",
    "'Según el autor, ¿Cómo el aprendizaje experiencial contribuye a los objetivos de un laboratorio de investigación en ciencias empresariales?'\n",
    "]\n",
    "\n",
    "for p in preguntas_lab:\n",
    "    query = p\n",
    "    print(query + '\\n')\n",
    "    llm_response = rag_chain_with_source.invoke(query)\n",
    "    process_llm_response(llm_response)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ACT LAB single question ###\n",
    "\n",
    "query = input(\"Hazme una pregunta: \")\n",
    "print(query + '\\n')\n",
    "llm_response = rag_chain_with_source.invoke(query)\n",
    "process_llm_response(llm_response)\n",
    "\n",
    "# Según el autor, ¿qué actividades pueden desarrollarse en un laboratorio de investigación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## UNWRAPER ###\n",
    "###############\n",
    "\n",
    "wrapped_text = input('Ingresa el texto a desenvolver: ')\n",
    "unwrapped_text = textwrap.fill(wrapped_text, width=10000)\n",
    "pyperclip.copy(unwrapped_text)\n",
    "print('Texto desenvuelto copiado en el portapapeles.\\n')\n",
    "print(unwrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacer historial y guardar conversaciones para medir efectividad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
