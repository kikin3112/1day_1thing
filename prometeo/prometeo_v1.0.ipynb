{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROMETEO_v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paquetería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "import textwrap\n",
    "import tiktoken\n",
    "import umap.umap_ as umap\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Cuenta el número de tokens en el documento\n",
    "    proporcionado.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def reduce_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Esta función no la comprendo. Estudie\n",
    "    para entenderla.\"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "def get_optimal_clusters(embeddings: np.ndarray, max_clusters: int = 50, random_state: int = 1234):\n",
    "    \"\"\"Obtiene el número óptimo de clústers.\"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    bics = [GaussianMixture(n_components=n, random_state=random_state).fit(embeddings).bic(embeddings)\n",
    "            for n in range(1, max_clusters)]\n",
    "    return np.argmin(bics) + 1\n",
    "\n",
    "def gmm_clustering(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"Clusteriza con el método GMM.\"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state).fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "def format_cluster_texts(df):\n",
    "    \"\"\"Agrupa los textos de cada clúster el listas.\"\"\"\n",
    "    clustered_texts = {}\n",
    "    for cluster in df['Cluster'].unique():\n",
    "        cluster_texts = df[df['Cluster'] == cluster]['Texto'].tolist()\n",
    "        clustered_texts[cluster] = \" --- \".join(cluster_texts)\n",
    "    return clustered_texts\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=80):\n",
    "    \"\"\"Formato para respuestas.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    \"\"\"Generador de referencias.\"\"\"\n",
    "    print(wrap_text_preserve_newlines(llm_response['answer']))\n",
    "    print('\\nReferencias:')\n",
    "    for contexto in llm_response[\"context\"][:5]:\n",
    "        print(contexto)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "embeddings = OpenAIEmbeddings(\n",
    ")\n",
    "\n",
    "detailed_turbo_llm = turbo_llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "template = \"\"\"Tu tarea es generar un resumen extremadamente detallado del siguiente\n",
    "texto: {text} \"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | detailed_turbo_llm | StrOutputParser()\n",
    "\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0.5,\n",
    "    model_name='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Quieres crear o cargar un tema de conversación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elegiste cargar un tema de conversación ya creado.\n",
      "\n",
      "Estás usando el tema de conversación: ascolfa_emb.txt\n",
      "\n",
      "K final es: 31\n",
      "\n",
      "\n",
      "Prometeo (1): Revisor bibliográfico.\n",
      "Yves (2): Diseñador de cartillas.\n",
      "Alan (3): Desarrollador de cartillas. \n",
      "Demós (4): Diseñador de presentaciones.\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"¿Crear (1) o cargar (2) un tema de conversación?: \")\n",
    "\n",
    "if user_input.lower() == \"1\":\n",
    "    print(f'Elegiste crear de cero un tema de conversación.\\n')\n",
    "\n",
    "    # Asegúrate de que haya PDFs en la carpeta 'docs'\n",
    "    documents = DirectoryLoader('./docs/', glob=\"./*.pdf\", loader_cls=PyPDFLoader).load()\n",
    "    # Tratameinto de caracteres indeseados\n",
    "    for d in documents:\n",
    "        d.page_content = d.page_content.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    texts = [doc.page_content for doc in docs]\n",
    "\n",
    "    d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "    d_reversed = list(reversed(d_sorted))\n",
    "    concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "        [doc.page_content for doc in d_reversed]\n",
    "    )\n",
    "    print(\n",
    "        \"Número de tokens en el documento proporcionado: %s\"\n",
    "        % num_tokens_from_string(concatenated_content)\n",
    "    )\n",
    "\n",
    "    global_embeddings = [embeddings.embed_query(txt) for txt in texts]\n",
    "\n",
    "    topic_name = input('¿Cómo se llama el tema de conversación?')\n",
    "\n",
    "    embed_name = topic_name + '_emb' + '.txt'\n",
    "    with open(rf'./{embed_name}', 'w') as f:\n",
    "        for i in global_embeddings:\n",
    "            f.write(\"%s\\n\" % i)\n",
    "    print(f'\\nEstás usando el tema de conversación: {embed_name}')\n",
    "\n",
    "    dim = 2\n",
    "    global_embeddings_reduced = reduce_cluster_embeddings(global_embeddings, dim)\n",
    "    labels, _ = gmm_clustering(global_embeddings_reduced, threshold=0.5)\n",
    "    simple_labels = [label[0] if len(label) > 0 else -1 for label in labels]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Texto': texts,\n",
    "        'Embedding': list(global_embeddings_reduced),\n",
    "        'Cluster': simple_labels\n",
    "    })\n",
    "\n",
    "    clustered_texts = format_cluster_texts(df)\n",
    "    summaries = {}\n",
    "    for cluster, text in clustered_texts.items():\n",
    "        summary = chain.invoke({\"text\": text})\n",
    "        summaries[cluster] = summary\n",
    "    embedded_summaries = [embeddings.embed_query(summary) for summary in summaries.values()]\n",
    "    embedded_summaries_np = np.array(embedded_summaries)\n",
    "    labels, _ = gmm_clustering(embedded_summaries_np, threshold=0.5)\n",
    "    simple_labels = [label[0] if len(label) > 0 else -1 for label in labels]\n",
    "    clustered_summaries = {}\n",
    "    for i, label in enumerate(simple_labels):\n",
    "        if label not in clustered_summaries:\n",
    "            clustered_summaries[label] = []\n",
    "        clustered_summaries[label].append(list(summaries.values())[i])\n",
    "    final_summaries = {}\n",
    "    for cluster, texts in clustered_summaries.items():\n",
    "        combined_text = ' '.join(texts)\n",
    "        summary = chain.invoke({\"text\": combined_text})\n",
    "        final_summaries[cluster] = summary\n",
    "    texts_from_df = df['Texto'].tolist()\n",
    "    texts_from_clustered_texts = list(clustered_texts.values())\n",
    "    texts_from_final_summaries = list(final_summaries.values())\n",
    "\n",
    "    combined_texts = texts_from_df + texts_from_clustered_texts + texts_from_final_summaries\n",
    "\n",
    "    file_name = topic_name + '.txt'\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        for t in combined_texts:\n",
    "            f.write(\"%s\\n\" % t)\n",
    "\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    textos = text_splitter.split_text(content)\n",
    "\n",
    "    persist_directory = topic_name + '_kb'\n",
    "    vectorstore = Chroma.from_texts(texts=textos,\n",
    "                                    embedding=embeddings,\n",
    "                                    persist_directory=persist_directory)\n",
    "    vectorstore.persist()\n",
    "    vectorstore = None\n",
    "    os.system(f'zip -r db.zip ./{persist_directory}')\n",
    "\n",
    "    vectorstore = Chroma(persist_directory=persist_directory,\n",
    "                         embedding_function=embeddings)\n",
    "\n",
    "    def adjust_final_number(string: str, max_threshold: int, initial_number: int) -> int:\n",
    "        final_number = initial_number\n",
    "        while final_number < max_threshold:\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": final_number})\n",
    "            docs = retriever.invoke(string)\n",
    "            text = \"\".join([doc.page_content for doc in docs])\n",
    "            if num_tokens_from_string(text) < max_threshold:\n",
    "                final_number += 1\n",
    "            else:\n",
    "                break\n",
    "        return final_number\n",
    "\n",
    "    final_number = adjust_final_number(\"¿Cuál es el tema principal del documento?\", 10000, 4)\n",
    "    print(f'\\nK final es: {final_number}')\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": final_number})\n",
    "    \n",
    "elif user_input.lower() == \"2\":\n",
    "    print('Elegiste cargar un tema de conversación ya creado.\\n')\n",
    "    global_embeddings = []\n",
    "\n",
    "    topic_name = input('¿Cómo se llama el tema de conversación?')\n",
    "\n",
    "    embed_name = topic_name + '_emb' + '.txt'\n",
    "    print(f'Estás usando el tema de conversación: {embed_name}\\n')\n",
    "    with open(rf'./{embed_name}', 'r') as f:\n",
    "        for i in f:\n",
    "            x = ast.literal_eval(i.strip())  # Convertir la cadena a lista de números\n",
    "            global_embeddings.append(x)\n",
    "\n",
    "    global_embeddings = np.array(global_embeddings, dtype=float)\n",
    "\n",
    "    file_name = topic_name + '.txt'\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    textos = text_splitter.split_text(content)\n",
    "\n",
    "    persist_directory = topic_name + '_kb'\n",
    "    vectorstore = Chroma(persist_directory=persist_directory, \n",
    "                    embedding_function=embeddings)\n",
    "\n",
    "    def adjust_final_number(string: str, max_threshold: int, initial_number: int) -> int:\n",
    "        final_number = initial_number\n",
    "        while final_number < max_threshold:\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": final_number})\n",
    "            docs = retriever.invoke(string)\n",
    "            text = \"\".join([doc.page_content for doc in docs])\n",
    "            if num_tokens_from_string(text) < max_threshold:\n",
    "                final_number += 1\n",
    "            else:\n",
    "                break\n",
    "        return final_number\n",
    "\n",
    "    final_number = adjust_final_number(\"¿Cuál es el tema principal del documento?\", 10000, 4)\n",
    "    print(f'K final es: {final_number}')\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": final_number})\n",
    "    \n",
    "elif user_input != \"1\" and user_input != \"2\":\n",
    "    print('No seleccionaste ningún tema de conversación.\\n')\n",
    "\n",
    "# Se personaliza el LLM #\n",
    "\n",
    "print('\\n\\nPrometeo (1): Revisor bibliográfico.\\nYves (2): Diseñador de cartillas.\\nAlan (3): Desarrollador de cartillas. \\nDemós (4): Diseñador de presentaciones.')\n",
    "\n",
    "template_select = input('Prometeo (1); Yves (2); Alan (3); Demós (4): ')\n",
    "\n",
    "if template_select == \"1\":\n",
    "\n",
    "    name = 'Prometeo'\n",
    "\n",
    "    template = \"\"\"\n",
    "    Eres Prometeo, un asistente personal especializado en revisión biliográfica que habla Español.\n",
    "    Tu tarea consiste en proporcionar respuestas extremadamente detalladas a cualquier tipo de \n",
    "    pregunta relacionada con el siguiente contexto obtenido de un artículo científico: {context}.\n",
    "\n",
    "    SIEMPRE debes responder en Español, con excepción de nombres propios.\n",
    "\n",
    "    NUNCA hables específicamente del contexto proporcionado.\n",
    "\n",
    "    NUNCA hables de ti a menos de que la pregunta lo requiera.\n",
    "\n",
    "    Teniendo en cuenta TODO lo anterior, responde la siguiente pregunta: {question}\n",
    "    \"\"\"\n",
    "\n",
    "elif template_select == \"2\":\n",
    "\n",
    "    name = 'Yves'\n",
    "\n",
    "    template = \"\"\"\n",
    "    Eres Yves, un asistente personal que habla Español, especializado en el diseño y desarrollo de cartillas \n",
    "    pedagógicas para estudiantes universitarios. Tu tarea consiste en ayudar a los estudiantes a identificar \n",
    "    información relevante del siguiente contexto obtenido de un artículo científico: {context}.\n",
    "\n",
    "    SIEMPRE debes responder en Español, con excepción de nombres propios.\n",
    "\n",
    "    NUNCA hables específicamente del contexto proporcionado.\n",
    "\n",
    "    NUNCA hables de ti a menos de que la tarea lo requiera.\n",
    "\n",
    "    Teniendo en cuenta TODO lo anterior, apoya al equipo con la siguiente tarea: {question}\n",
    "    \"\"\"\n",
    "\n",
    "elif template_select == \"3\":\n",
    "\n",
    "    name = 'Alan'\n",
    "\n",
    "    template = \"\"\"\n",
    "    Eres Alan, un asistente personal especializado en el diseño y desarrollo de cartillas pedagógicas\n",
    "    visuales para estudiantes universitarios. Tu tarea es generar una propuesta de material didáctico en \n",
    "    formato de historieta, que cubra el tema principal en el siguiente contexto: {context}. El contenido\n",
    "    para cada página debe generarse del contexto proporcionado.\n",
    "\n",
    "    Los requisitos y condiciones esperadas de la propuesta son: \n",
    "    \n",
    "    - Total de 10 páginas con explicación de conceptos clave.\n",
    "    - Muy gráfico y profesional.\n",
    "    - Combinar ilustraciones con información clave sobre el tema.\n",
    "    - Cada página debe mantener un equilibrio entre el contenido visual y la explicación de los conceptos.\n",
    "    - Tener recursos pedagógicos como \"tips\" o secciones de \"¿sabías qué?\" con información detallada.\n",
    "\n",
    "    SIEMPRE debes responder en Español, con excepción de nombres propios.\n",
    "\n",
    "    NUNCA hables específicamente del contexto proporcionado.\n",
    "\n",
    "    NUNCA hables de ti a menos de que la tarea lo requiera.\n",
    "\n",
    "    Teniendo en cuenta TODO lo anterior, apoya al usuario con la siguiente tarea: {question}\n",
    "    \"\"\"\n",
    "\n",
    "elif template_select == \"4\":\n",
    "\n",
    "    name = 'Demós'\n",
    "    \n",
    "    template = \"\"\"\n",
    "    Eres Demós, un asistente personal experto en la creación y diseño de presentaciones académicas para \n",
    "    investigadores y estudiantes universitarios. Tu tarea es desarrollar propuesta de presentación clara y efectiva \n",
    "    que aborde el tema del siguiente contexto: {context}. El contenido para cada diapositiva debe generarse \n",
    "    del contexto proporcionado.\n",
    "\n",
    "    Los requisitos y condiciones esperadas para la presentación son:\n",
    "\n",
    "    - Total de 10 diapositivas.\n",
    "    - Diseño profesional y visualmente atractivo.\n",
    "    - Incluir gráficos o ilustraciones relevantes que apoyen el contenido.\n",
    "    - Cada diapositiva debe equilibrar texto e imágenes, evitando la sobrecarga de información.\n",
    "    - Debe incluir secciones como \"Introducción\", \"Desarrollo del tema\", \"Conclusiones\", y \"Referencias\".\n",
    "    - Opcionalmente, puedes agregar diapositivas con \"Preguntas clave\" para facilitar la discusión.\n",
    "\n",
    "    SIEMPRE debes responder en Español, con excepción de nombres propios.\n",
    "\n",
    "    NUNCA hables específicamente del contexto proporcionado.\n",
    "\n",
    "    NUNCA hables de ti a menos que la tarea lo requiera.\n",
    "\n",
    "    Teniendo en cuenta TODO lo anterior, apoya al usuario con la siguiente tarea: {question}\n",
    "    \"\"\"\n",
    "\n",
    "else:\n",
    "    print('No seleccionaste ningún asistente.\\n')\n",
    "\n",
    "prometeo_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prometeo_prompt\n",
    "    | turbo_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ask_questions():\n",
    "#     while True:\n",
    "#         if template_select == \"1\":\n",
    "#             query = input(f\"¡Hola, soy {name}! ¿Qué pregunta tienes?: \")\n",
    "#             print(query)\n",
    "#             llm_response = rag_chain_with_source.invoke(query)\n",
    "#             process_llm_response(llm_response)\n",
    "#             print('\\n\\n')\n",
    "            \n",
    "#             another_question = input(\"¿Tienes otra pregunta? (s/n): \").strip().lower()\n",
    "#             if another_question != 's':\n",
    "#                 break\n",
    "#         elif template_select == \"2\":\n",
    "#             query = input(f\"¡Hola, soy {name}! ¿En qué te puedo servir?: \")\n",
    "#             print(query)\n",
    "#             llm_response = rag_chain_with_source.invoke(query)\n",
    "#             process_llm_response(llm_response)\n",
    "#             print('\\n\\n')\n",
    "            \n",
    "#             another_question = input(\"¿Te puedo servir en algo más? (s/n): \").strip().lower()\n",
    "#             if another_question != 's':\n",
    "#                 break\n",
    "#         else:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prompt: Genera un mapa mental con los puntos claves del documento.\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ask_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prometeo_preguntas_meta = ['¿Cuál es el título original y traducido del artículo?',\n",
    "#                            '¿Quién o quiénes son los autores del artículo?',\n",
    "#                            '¿En qué año fue redactado el artículo?',\n",
    "#                            '¿En qué revista o medio fue publicado el artículo?',\n",
    "#                            '¿En qué país fue redactado el artículo?',\n",
    "#                            '¿Puedes generar una cita bibliográfica en formato APA del documento?',\n",
    "#                            '¿Puedes hacer un resumen muy breve y conciso del documento?',\n",
    "#                            '¿Cuál es el link al DOI del documento?'\n",
    "                  \n",
    "                  \n",
    "#                   #'¿Cuáles son las palabras clave de investigación mencionadas en el documento y traducidas al español, del artículo?'\n",
    "#                   #'¿Cuál es la idea principal de investigación?',\n",
    "#                   #'¿Cuál es el objetivo u objetivos de la investigación?',\n",
    "#                   #'¿Cuál es la metodología usada en la investigación?',\n",
    "#                   #'¿Cuáles fueron los resultados más importantes de la investigación?',\n",
    "#                   #'¿Los resultados cumplen los objetivos de investigación?',\n",
    "#                   #'¿Cómo puede aportar esta investigación al diseño, desarrollo y puesta en marcha de un laboratorio de investigación para el programa de Administración de Empresas en una universidad?'\n",
    "# ]\n",
    "\n",
    "# for p in prometeo_preguntas_meta:\n",
    "#     query = p\n",
    "#     print(query)\n",
    "#     llm_response = rag_chain_with_source.invoke(query)\n",
    "#     process_llm_response(llm_response)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prometeo_preguntas_ddl = ['¿Cómo clasificarías el contenido del documento: 1) Explicación de la importancia del diseño de laboratorios de investigación o 2) Hallazgos importantes sobre el diseño de laboratorios de investigación? ¿Por qué?',\n",
    "#                           'De manera resumida, ¿qué puede aportar el artículo para el diseño y desarrollo de un laboratorio de investigación para el programa de Administración en una universidad?',\n",
    "#                           '¿Puedes generar un cita en formato APA del documento?'\n",
    "# ]\n",
    "\n",
    "# for p in prometeo_preguntas_ddl:\n",
    "#     query = p\n",
    "#     print(query)\n",
    "#     llm_response = rag_chain_with_source.invoke(query)\n",
    "#     process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Demo clasi\n",
    "# ddl = '¿Cómo clasificarías el contenido del documento: 1) Contenido introductorio que explica la importancia del diseño de laboratorios de investigación o 2) Hallazgos importantes sobre el diseño de laboratorios de investigación? ¿Por qué?'\n",
    "# justi = '¿El contenido podría brindar soporte a la hipótesis que afirma que los graduados de la universidad no poseen las habilidades y competencias necesarias para enfrentarse al mercado laboral actual? Si lo hace, menciona el punto más importante para reforzar la idea.'\n",
    "# query = ddl\n",
    "# print(query)\n",
    "# llm_response = rag_chain_with_source.invoke(query)\n",
    "# process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prometeo_preguntas_pedagogia = ['¿Cuál es la idea principal del artículo?',\n",
    "#                                 '¿Cuáles son los puntos claves del artículo que me pueden ayudar a diseñar material pedagógico efectivo?',\n",
    "#                                 'Según el artículo, ¿qué características deberían tener los materiales pedagógicos efectivos?',\n",
    "#                                 '¿Puedes generar un cita en formato APA del documento?'\n",
    "# ]\n",
    "\n",
    "# for p in prometeo_preguntas_pedagogia:\n",
    "#     query = p\n",
    "#     print(query)\n",
    "#     llm_response = rag_chain_with_source.invoke(query)\n",
    "#     process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yves_tareas_ls = ['Menciona principios básicos relacionados con el Lean Startup.',\n",
    "#                   'Menciona ejemplos de aplicación práctica del Lean Startup',\n",
    "#                   'Menciona herramientas y métodos para la aplicación del Lean Startup.',\n",
    "#                   'Identifica errores que se deben evitar en el Lean Startup.',\n",
    "#                   '¿El documento incluye casos de éxito en el uso del Lean Startup? Menciónalos.',\n",
    "#                   'Dime ¿cómo puede aportar el artículo para el diseño de una cartilla pedagógica basada en Lean Startup?',\n",
    "#                   'Genera una cita en formato APA del documento.'\n",
    "# ]\n",
    "\n",
    "# for p in yves_tareas_ls:\n",
    "#     query = p\n",
    "#     print(query + '\\n')\n",
    "#     llm_response = rag_chain_with_source.invoke(query)\n",
    "#     process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if template_select == '1':\n",
    "#     query = input(\"Hazme una pregunta: \")\n",
    "#     print(query + '\\n')\n",
    "#     llm_response = rag_chain_with_source.invoke(query)\n",
    "#     process_llm_response(llm_response)\n",
    "# elif template_select == \"2\":\n",
    "#     query = input(\"¿Cuál es mi tarea?: \")\n",
    "#     print(query + '\\n')\n",
    "#     llm_response = rag_chain_with_source.invoke(query)\n",
    "#     process_llm_response(llm_response)\n",
    "# elif template_select == \"3\":\n",
    "#     query = input(\"¿Cuál es mi tarea?: \")\n",
    "#     print(query + '\\n')\n",
    "#     llm_response = rag_chain_with_source.invoke(query)\n",
    "#     process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿El documento puede aportar a la definición de 'herramientas de inteligencia artificial'?\n",
      "\n",
      "Sí, el documento puede aportar significativamente a la definición de\n",
      "'herramientas de inteligencia artificial', ya que aborda diversos aspectos\n",
      "relacionados con la inteligencia artificial generativa (IA generativa) y su\n",
      "aplicación en el ámbito educativo, especialmente en la educación emprendedora. A\n",
      "continuación, se detallan algunos puntos clave que pueden enriquecer esta\n",
      "definición:\n",
      "\n",
      "1. **Generación de Contenido**: Las herramientas de inteligencia artificial,\n",
      "como los modelos generativos, están diseñadas para crear nuevo contenido o datos\n",
      "que se ajusten a las distribuciones de probabilidad de un conjunto de datos\n",
      "dado. Esto incluye la generación de texto, imágenes, audio y otros tipos de\n",
      "datos, lo que demuestra su capacidad para producir resultados creativos y útiles\n",
      "en diferentes contextos.\n",
      "\n",
      "2. **Asistencia en Tareas Académicas**: Estas herramientas son utilizadas para\n",
      "facilitar tareas académicas, como la búsqueda de artículos, el análisis de datos\n",
      "y la asistencia en la escritura. Esto implica que las herramientas de IA no solo\n",
      "generan contenido, sino que también ayudan a los usuarios a interactuar con la\n",
      "información de manera más eficiente.\n",
      "\n",
      "3. **Mejora del Aprendizaje**: Las herramientas de inteligencia artificial\n",
      "pueden mejorar los resultados de aprendizaje al ofrecer experiencias\n",
      "personalizadas y adaptadas a las necesidades individuales de los estudiantes.\n",
      "Esto se traduce en un enfoque más centrado en el estudiante, donde la IA puede\n",
      "ayudar a desarrollar habilidades críticas como el pensamiento crítico y la auto-\n",
      "reflexión.\n",
      "\n",
      "4. **Integración en Procesos Empresariales**: En el contexto empresarial, estas\n",
      "herramientas pueden ser aplicadas en diversas funciones, como marketing,\n",
      "finanzas y desarrollo de productos. Esto subraya la versatilidad de la IA en\n",
      "diferentes sectores y su capacidad para transformar procesos y metodologías.\n",
      "\n",
      "5. **Desafíos Éticos y de Uso**: El documento también menciona las\n",
      "preocupaciones éticas relacionadas con el uso de herramientas de IA, como la\n",
      "transparencia de los algoritmos, los sesgos en los datos y la privacidad. Estos\n",
      "aspectos son fundamentales para definir y regular el uso responsable de la\n",
      "inteligencia artificial en diferentes aplicaciones.\n",
      "\n",
      "6. **Innovación y Experimentación**: Finalmente, se destaca la importancia de\n",
      "innovar y experimentar con estas herramientas para maximizar sus beneficios y\n",
      "minimizar sus riesgos. Esto sugiere que las herramientas de IA son dinámicas y\n",
      "deben ser adaptadas y evaluadas continuamente en función de su impacto en los\n",
      "usuarios.\n",
      "\n",
      "En resumen, el documento proporciona una visión amplia y matizada de las\n",
      "herramientas de inteligencia artificial, resaltando su potencial, aplicaciones y\n",
      "desafíos, lo que puede contribuir a una definición más completa y\n",
      "contextualizada de este concepto.\n",
      "\n",
      "Referencias:\n",
      "page_content='El editorial titulado \"Entrepreneurship Education at the Dawn of Generative Artificial Intelligence\", publicado en 2023 en la revista *Entrepreneurship Education and Pedagogy*, examina la rápida evolución de la inteligencia artificial generativa (IA generativa) y su creciente influencia en la educación emprendedora. Los autores, Christoph Winkler, Basel Hammoda, Erik Noyes y Marco Van Gelderen, argumentan que esta transformación tecnológica representa un cambio de paradigma similar al surgimiento de Internet.'\n",
      "page_content='### Contexto y Relevancia\\nLos educadores en emprendimiento están en una posición crítica, ya que deben integrar estas innovaciones en su enseñanza y aprovechar su potencial para transformar las dinámicas de aprendizaje. El editorial hace un llamado a la comunidad educativa para innovar, experimentar y aprender sobre el impacto de la IA generativa en su disciplina. Se proponen preguntas de investigación que guiarán a la comunidad en la exploración de este nuevo campo.\\n\\n### Impacto de ChatGPT y Herramientas de IA\\nEl lanzamiento de ChatGPT por OpenAI en noviembre de 2022 marcó un hito, convirtiéndose en el chatbot más rápidamente adoptado en la historia, con 1.8 mil millones de visitas en abril de 2023. Su adopción ha demostrado una amplia gama de aplicaciones, desde la creación de contenido hasta el desarrollo de software. Microsoft, al integrar ChatGPT en Bing, reportó un aumento significativo en el tráfico, evidenciando el impacto inmediato de la IA generativa.\\n\\nAdemás de ChatGPT, existen herramientas de IA especializadas en investigación y educación, que facilitan tareas académicas como la búsqueda de artículos, el análisis de datos y la asistencia en la escritura. Sin embargo, también surgen preocupaciones sobre el uso indebido de estas herramientas por parte de los estudiantes, lo que plantea desafíos significativos en el ámbito educativo.\\n\\n### Desafíos y Oportunidades en la Educación Emprendedora\\nEl texto destaca que, aunque la adopción de herramientas de IA es reciente, muchas empresas ya las están integrando en sus operaciones. Los educadores en emprendimiento deben adaptarse a esta innovación, no solo enseñando sobre la IA, sino también utilizando su potencial para reformar sus métodos de enseñanza. Se enfatiza que ni la prohibición ni la adopción acrítica de la IA son soluciones efectivas; en cambio, se sugiere un enfoque crítico que aumente la comprensión y las capacidades conceptuales.'\n",
      "page_content='### Marco Conceptual y Preguntas de Investigación\\nEl editorial introduce un marco conceptual que ayuda a educadores, investigadores y practicantes a comprender el impacto de la IA generativa en la educación emprendedora. Se proponen preguntas clave para guiar la investigación, como cómo los emprendedores pueden aprovechar la IA generativa para crear valor, cómo está evolucionando su comportamiento y cognición, y cómo la IA puede reconfigurar funciones empresariales clave.\\n\\n### Implicaciones para el Futuro\\nLa IA generativa no solo transformará la enseñanza y el aprendizaje en el ámbito empresarial, sino que también influirá en la investigación y el desarrollo de nuevas metodologías educativas. Se concluye con un llamado a la acción para que la comunidad educativa adopte un enfoque proactivo en la integración de la IA generativa en sus prácticas y teorías.\\n\\n### Consideraciones Éticas y Prácticas\\nEl texto también aborda cuestiones éticas relacionadas con la IA, como la transparencia de los algoritmos, los sesgos en los datos, y la privacidad y seguridad de la información. Se enfatiza la necesidad de un enfoque reflexivo y crítico en la implementación de estas tecnologías, asegurando que se utilicen de manera responsable y equitativa.\\n\\n### Conclusión\\nEn resumen, el editorial subraya la importancia de adaptarse a la revolución que representa la IA generativa en la educación emprendedora, promoviendo un enfoque equilibrado que maximice los beneficios y minimice los riesgos asociados con su uso. Se destaca la necesidad de innovar en la educación emprendedora mediante el uso de la IA generativa, considerando la personalización del aprendizaje, la preparación de los educadores y la evaluación de resultados como elementos clave para mejorar las habilidades de los estudiantes.'\n",
      "page_content='ﬁeld forward by exploring, trying (and failing), and investigating this new “humanized ”technology. We hope our framework and associated inquiry questions will support and advance exc iting lines of inquiry and facilitate a larger conversation about opportunities and challenges presented by advances in generative AIthat were previously inconceivable. Acknowledgments The authors would like to thank Dr. Eric Liguori who provided helpful feedback on the paper draft. Declaration of Con ﬂicting Interests The author(s) declared no potential con ﬂicts of interest with respect to the research, authorship, and/or publication of this article.Winkler et al. 587'\n",
      "page_content='ﬁeld forward by exploring, trying (and failing), and investigating this new “humanized ”technology. We hope our framework and associated inquiry questions will support and advance exc iting lines of inquiry and facilitate a larger conversation about opportunities and challenges presented by advances in generative AIthat were previously inconceivable. Acknowledgments The authors would like to thank Dr. Eric Liguori who provided helpful feedback on the paper draft. Declaration of Con ﬂicting Interests The author(s) declared no potential con ﬂicts of interest with respect to the research, authorship, and/or publication of this article.Winkler et al. 587'\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### ACT ASCOLFA ###\n",
    "\n",
    "if template_select == '1':\n",
    "    query = input(\"Hazme una pregunta: \")\n",
    "    print(query + '\\n')\n",
    "    llm_response = rag_chain_with_source.invoke(query)\n",
    "    process_llm_response(llm_response)\n",
    "elif template_select == \"2\":\n",
    "    query = input(\"¿Cuál es mi tarea?: \")\n",
    "    print(query + '\\n')\n",
    "    llm_response = rag_chain_with_source.invoke(query)\n",
    "    process_llm_response(llm_response)\n",
    "elif template_select == \"3\":\n",
    "    query = input(\"¿Cuál es mi tarea?: \")\n",
    "    print(query + '\\n')\n",
    "    llm_response = rag_chain_with_source.invoke(query)\n",
    "    process_llm_response(llm_response)\n",
    "elif template_select == \"4\":\n",
    "    query = input(\"¿Cuál es mi tarea?: \")\n",
    "    print(query + '\\n')\n",
    "    llm_response = rag_chain_with_source.invoke(query)\n",
    "    process_llm_response(llm_response)\n",
    "else:\n",
    "    print(\"Para realizar una petición, primero debes seleccionar una asistente.\")\n",
    "\n",
    "## ¿Cómo puede aportar el documento a una propuesta de investigación para el diseño de un curso sobre herramientas de inteligencia artificial?\n",
    "# ¿Puedes generar una cita en formato APA del documento?\n",
    "# ¿El documento puede aportar a la definición de 'herramientas de inteligencia artificial'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué recomendaciones me puesdes dar para fortalecer la propuesta de investigación?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if template_select == '1':\n",
    "    query = input(\"Hazme una pregunta: \")\n",
    "    print(query + '\\n')\n",
    "    llm_response = rag_chain_with_source.invoke(query)\n",
    "    process_llm_response(llm_response)\n",
    "elif template_select == \"2\":\n",
    "    query = input(\"¿Cuál es mi tarea?: \")\n",
    "    print(query + '\\n')\n",
    "    llm_response = rag_chain_with_source.invoke(query)\n",
    "    process_llm_response(llm_response)\n",
    "elif template_select == \"3\":\n",
    "    query = input(\"¿Cuál es mi tarea?: \")\n",
    "    print(query + '\\n')\n",
    "    llm_response = rag_chain_with_source.invoke(query)\n",
    "    process_llm_response(llm_response)\n",
    "elif template_select == \"4\":\n",
    "    query = input(\"¿Cuál es mi tarea?: \")\n",
    "    print(query + '\\n')\n",
    "    llm_response = rag_chain_with_source.invoke(query)\n",
    "    process_llm_response(llm_response)\n",
    "else:\n",
    "    print(\"Para realizar una petición, primero debes seleccionar una asistente.\")\n",
    "\n",
    "    # ¿Cómo aporta el artículo a una presentación basada en el tema: 'Burocracia y su relación con las organizaciones'?\n",
    "\n",
    "# ¿Puedes generar una cita en formato APA del documento?\n",
    "\n",
    "# ¿Qué recomendaciones me puesdes dar para fortalecer la propuesta de investigación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las herramientas de inteligencia artificial, como los modelos generativos, están diseñadas para crear nuevo contenido o datos que se ajusten a las distribuciones de probabilidad de un conjunto de datos dado. Esto incluye la generación de texto, imágenes, audio y otros tipos de datos, lo que demuestra su capacidad para producir resultados creativos y útiles en diferentes contextos.\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "## UNWRAPER ###\n",
    "###############\n",
    "\n",
    "wrapped_text = input('Ingresa el texto a desenvolver: ')\n",
    "unwrapped_text = textwrap.fill(wrapped_text, width=10000)\n",
    "pyperclip.copy(unwrapped_text)\n",
    "print(unwrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#busque caracteristicas\n",
    "\n",
    "\n",
    "¿Cuáles son los puntos claves del artículo que me pueden ayudar a diseñar una cartilla pedagógica basada en 'Lean Startup' como tema principal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacer historial y guardar conversaciones para medir efectividad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
