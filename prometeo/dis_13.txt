Student Teaching and Research Laboratory Focusing on Brain–computer Interface Paradigms – A Creative Environment for Computer Science Students – Tomasz M. Rutkowski1,2 Abstract — This paper presents an applied concept of a brain– computer interface (BCI) student research laboratory (BCI– LAB) at the Life Science Center of TARA, University of Tsukuba, Japan. Several successful case studies of the student projects are reviewed together with the BCI Research Award 2014 winner case. The BCI–LAB design and project–based teaching philosophy is also explained. Future teaching and research directions summarize the review. I. INTRODUCTION Brain computer interface (BCI) is a technology that uses brain neural activities only to control a computer, or a machine, without any body muscle movements [1]. This technology could allow disabled people, such as locked–in syndrome (LIS) suffering patients [2], to regain communica- tion skills or to manage daily life support related functions. On the other hand, the BCI technology creates a perfect platform for computer science students to study computa- tional neuroscience–based neurotechnology methods. There are also many types of BCIs based on the utilized brainwave signals and experimental paradigms. This allows to create the multi–sensory experimental labs with biomedical (e.g. neu- rophysiological) signals capturing and processing in mind. Creation of multi–sensory stimuli allows also the students to practically develop and test multimedia environments for a future deployment of the developed paradigms for paralyzed patients. This review paper presents a successful implementation of the BCI teaching and experimental lab at the Life Science Center of TARA, University of Tsukuba, Japan. The experimental focus of the non–invasive BCI lab is on the external stimulus–driven paradigms relaying on event related potential’s (ERP) P300 responses [1] allowing for multi–sensory stimulation application. The sensory modali- ties available for
[1] allowing for multi–sensory stimulation application. The sensory modali- ties available for experimenting are as follows, (i) spatial au- ditory: with multi–loudspeaker studio conﬁgurations [3]–[8] and headphone viral sound–based [9], [10]; (ii) spatial visual: P300 [4] and steady–state response–based [11], [12]; (iii) spatial tactile: realized with vibration–based P300 [13]–[15], tactile–force [16], pin–pressure [17], and airborne ultrasonic ∗The presented student research and educational laboratory was supported in part by the Strategic Information and Communications R&D Promotion Program (SCOPE) no. 121803027 of The Ministry of Internal Affairs and Communication in Japan, YAMAHA Corporation and by KAKENHI, the Japan Society for the Promotion of Science, grant no. 24243062 . 1Tomasz M. Rutkowski is with Life Science Center of TARA, Uni- versity of Tsukuba, 1-1-1 Tennodai, Tsukuba, Ibaraki, 305-8577 Japan, tomek@bci-lab.info http://bci-lab.info/ 2Tomasz M. Rutkowski is also with RIKEN Brain Science Institute, Wako-shi, Saitama, Japantactile display (AUTD) contactless somatosensory modal- ity [18], [19]. Non–invasive BCIs are the most safe because the brainwave sensors are usually attached to the surface of a human head, thus such experiments, within the institu- tional ethical committee guidelines, are perfectly suitable for computer science students. All the experiments reviewed and reported in this paper were conducted in agreement with the ethical committee guidelines of the Faculty of Engineering, Information and Systems at University of Tsukuba, Tsukuba, Japan (experimental permission no. 2013R7). The non– invasive BCI modalities, however, suffer usually from lower brainwave quality due to a very poor signal to noise ratio comparing to the invasive systems, which creates a perfect educational project platform for the engineering students, who design ﬁltering and feature extraction algorithms to cope with the above problems [15], [20], [21]. The
ﬁltering and feature extraction algorithms to cope with the above problems [15], [20], [21]. The non–invasive BCIs are divided into stimulus–driven. The stimulus–driven BCIs constitute the interactive interfacing solutions in which the commands to operate a computer or an application are associated with sensory stimuli delivered to the user. The user generates a signature brainwave captured in EEG by focusing attention (on the so–called target eliciting the P300) or by ignoring (the non–target ) the randomly presented sensory auditory, tactile or visual stimuli. The students of the BCI– LAB design those spatial sensory stimulations and enhance practically their multimedia knowledge with support of the neurotechnology (brain response–driven) applications. Among the recently developed BCIs the visual modal- ity [1] is currently the most successful because the evoked P300 responses have usually the largest amplitudes allowing for the most easy classiﬁcation [4]. However, it is known that LIS users (e.g. advanced stage amyotrophic lateral sclerosis patients) cannot use visual modality because they gradually lose intentional muscle control, including eye movements, focusing or intentional blinking. Our research hypothesis is that the auditory and tactile modalities BCI shall be a more suitable solution for communication establishment for LIS users. The students of the BCI–LAB have an opportunity to design “novel sensory stimulations” by combining tactile and bone–conduction–auditory [22]; audiovisual [4]; and contactless tactile [18], [19] paradigms. The rest of the paper is organized as follows. In the next section we describe the teaching philosophy behind the BCI–LAB concept. Next the experimental environment is described focusing on the hardware, software and stimulus generators. Finally the already published student projects978-1-4244-9270-1/15/$31.00 ©2015 IEEE3667 Authorized licensed use limited to: Universidad Nacional de Colombia (UNAL). Downloaded on July
Authorized licensed use limited to: Universidad Nacional de Colombia (UNAL). Downloaded on July 23,2024 at 22:20:41 UTC from IEEE Xplore.  Restrictions apply.
are brieﬂy described. Finally, the conclusions are drawn and future research and educational directions outlined. II. TEACHING PHILOSOPHY A teaching philosophy implemented in the BCI–LAB is based on illustrating the beauty of a neuroscience-driven creative innovation of media and human augmentation. Even though computational neuroscience and the modern media or human augmentation might be regarded as not very closely related research disciplines in the eyes of the general public, practical approaches often result with the both disciplines sharing outcomes from the both ﬁelds. For most the BCI– LAB students the projects are their ﬁrst chance to study brain-related subjects in application to real-life multimedia and human augmentation applications’ development. To be- gin each new project, we use in the lab an analogy to real life examples to lead the students to the reasonability of the method. It happens that when we talk about the vision, hearing or some other perceptual methods we are ourselves so familiar with, we ﬁnd ourselves facing a room of confused faces. The best remedy for such initial confusion is a participation in as experimental subjects in ongoing lab projects usually run by the senior students. A master–student relationship between students could be simply realized, which helps with lab culture establishment. A base for an successful experimental lab foundation is an interactive and student–friendly teaching approach. No lab seminar notes can be recycled from one semester to the next, nor is it best to use the same examples. Students are different and their ways of developing understanding too. Our lab philosophy is that a senior experimenter should always be ready to adjust an advisory strategy due to the needs of the new students. If any new to the project student complains about the problem or a lack of available solution, a drill experimental example may become necessary; if there is one tricky problem in the experimental setting, a little hint
may become necessary; if there is one tricky problem in the experimental setting, a little hint given will make the students efforts more inspired; if the experimental design has a dull nature, adding some interesting trial examples may lighten it up and make the lab life more enjoyable and less slumberous. The concept of our BCI–LAB is to train the computer science students in computational neuroscience problems by using ”hands–on” practical problems’ learning philosophy. III. BCI–LAB ENVIRONMENT There are three pillars of a successful experimental stimulus–driven BCI–LAB, namely a good experimental hardware environment; ﬂexible software platforms for brain- wave signal processing and classiﬁcation; an excellent multi– sensory stimulus design platform. The experimental hardware, allowing for EEG capture for the online BCI experiments discussed in the following sec- tions, has been set up based on g.USBamp and g.MOBIlab+ from g.tec Medical Engineering, Austria (used in the major- ity of the projects in BCI–LAB), as well as on vAmp from Brain Products GmbH, Germany (used only in [7], [8]). All the systems used the active EEG electrodes g.LADYbird and g.SAHARA from g.tec Medical Engineering, Austria. Fig. 1. A photograph of a two–step input Japanese kana characters spatial auditory BCI [7], [8] project realized in the BCI–LAB. The user in the photograph sits in front of ten loudspeakers (sponsored by YAMAHA Corporation) delivering the spatial sound images. The yellow Japanese “na” character visualizes (not seen to the user) the sound image location. An online video from the experiment is available at [26]. Due to different EEG ampliﬁers manufacturers we decided to use, and to extend with own functions, ﬂexible signal acquisition and classiﬁcation environments BCI2000 [23] and OpenVibe [24]. The both environments allow for ﬂexible extensions also developed and applied by our team [11], [21]. Finally the spatial auditory, visual and tactile stimulation generators were
our team [11], [21]. Finally the spatial auditory, visual and tactile stimulation generators were programmed by our team in MAX 6 [25] and delivered to the users via multichannel sound cards, large computer displays or tactile transducers managed by ARDUINO micro–controller boards. IV. BCI–LAB STUDENT PROJECTS 1) Multi–loudspeaker–based Spatial Auditory BCIs: Three different multi–loudspeaker–based spatial auditory BCI paradigms were realized in the BCI–LAB allowing the students to design and test multi–source auditory environ- ments using MAX 6 visual programming environment [25]. The ﬁrst approach implemented a vector–based amplitude panning method to spatial horizontal sound images posi- tioned octagonally around the user’s head [3]. The second approach involved moving sound stimuli realized in half– spherical loudspeaker environments [5], [6]. The third, and the most successful application, realized a two–step–input speller based on the spatial auditory sound images pre- sentation [7], [8] as shown in Figure 1. The application has been developed for 45Japanese syllabary ( kana ) letters using a two–step–input procedure in order to create an easy to use BCI–speller interface. The results of the con- ducted online EEG experiments have shown the feasibility of the Japanese kana characters set in two–step–input saBCI– speller paradigm. Also we conﬁrmed that the accuracy of each modality had no signiﬁcant differences. A video doc- umenting the online saBCI–speller experiment is available at [26]. The three presented auditory BCI paradigms allowed for training of the computer science students in spatial sound design and biomedical signal acquisition (EEG), processing and classiﬁcation for the online applications. 2) Headphone–based Spatial Auditory BCI Paradigm: This study reported on a head related impulse response (HRIR) application to an auditory spatial brain-computer interface (BCI) speller paradigm [9], [10]. Six experienced and ﬁve BCI–naive users participated
interface (BCI) speller paradigm [9], [10]. Six experienced and ﬁve BCI–naive users participated in an experimental3668 Authorized licensed use limited to: Universidad Nacional de Colombia (UNAL). Downloaded on July 23,2024 at 22:20:41 UTC from IEEE Xplore.  Restrictions apply.
Fig. 2. A photograph of a chromatic SSVEP BCI project [11], [12] realized in the BCI–LAB. The user in the photograph sits in front of a frame with four green–blue LEDs. A humanoid robot is managed using four commands from the SSVEP BCI. An online video from the experiment is available at [27]. spelling set up based on ﬁve Japanese vowels. Obtained auditory evoked potentials resulted with encouragingly good and stable P300–responses in online BCI experiments. Our case study indicated that the headphone reproduced auditory (HRIR–based) spatial sound paradigm could be a viable alternative to the established multi–loudspeaker surround sound BCI–speller applications, as far as healthy pilot study users are concerned. This project allowed the students to practically apply headphone–based spatial sound virtualiza- tion to the online auditory BCI application [9], [10]. 3) Visual SSVEP and cVEP Paradigms: A novel approach to steady–state visual evoked potentials (SSVEP) based BCI was developed [11], [12] with further extension to a code– modulated visual evoked potential (cVEP) approach. To minimize possible side effects of the monochromatic light SSVEP we proposed to utilize color information in form of the green and blue interlaced ﬂicker (see Figure 2 with a robot control application). The feasibility of the proposed method was evaluated in a comparison of the classical monochromatic versus the proposed colorful SSVEP re- sponses. The proposed method scored with higher accuracies comparing to the monochromatic SSVEP [11]. This visual modality project allowed the student to attack the classical problems related to ﬂickering light–based stimuli (danger of photosensitive epilepsy, etc.). As result two solutions were proposed based on higher frequency and cVEP–based stimuli together with less sensitive chromatic light modality. 4) Bone–conduction Auditory and Tactile BCI Paradigm: In this project we studied the extent to which vibrotactile stimuli delivered to the head of a
In this project we studied the extent to which vibrotactile stimuli delivered to the head of a subject could serve as a platform for the BCI paradigm [22] (see Figure 3). Six head positions were used to evoke combined somatosensory and auditory (via bone-conduction effect) brain responses, in order to deﬁne a multimodal tactile and auditory brain computer interface (taBCI). Experimental results on subjects performing online taBCI, using stimuli with a moderately fast inter-stimulus-interval, validated the taBCI paradigm, while the feasibility of the concept was illuminated through encouraﬁng information-transfer rate case studies. In the reported experiments [22] only a single BCI-nave subject obtained 100% and also one obtained 0%for the six digit sequence spelling accuracy with 5-trials averaging procedure. Fig. 3. A photograph of a bone–conduction auditory and tactile multi– sensory BCI Paradigm [15], [22] project realized in the BCI–LAB. The user in the photograph manages a walking virtual avatar based on intentional responses to the combined tactile and bone–conduction auditory stimuli delivered thorough vibrotactile transducers embedded in the EEG cap (large metallic discs above the ear and on a forehead in the picture). The results were further improved with an implementation of a novel synchro–squeezing EEG preprocessing method as reported in [15]. This project allowed the BCI–LAB students for a creation of a novel multi–sensory BCI platform [22] together with the new signal processing method develop- ment [15]. 5) Vibrotactile and Tactile–pressure BCI Paradigms: A series of successful vibrotactile [13], [14] and tactile– pressure/force [16], [17] paradigms were developed with the BCI–LAB approaching a problem of communication alternatives design for those LIS patients with no vision or functional hearing (due to the so called ear stacking syndrome). In these spatial tactile projects the contributing students combined successfully their knowledge of multi-
spatial tactile projects the contributing students combined successfully their knowledge of multi- channel stimulus generation systems development in MAX 6 environment [25] together with brainwave processing and classiﬁcation. 6) Airborne Ultrasonic Tactile Display BCI Paradigm: In this BCI Research Award 2014 winning project a contact– less somatosensory stimuli were delivered via an airborne ultrasonic tactile display (AUTD) to the palms of a user (see Figure 4) and served as a platform for the BCI paradigm (autdBCI) [18], [19]. Six palm positions were used to evoke combined somatosensory brain responses, in order to deﬁne a novel contact-less tactile BCI. A comparison was made with classical attached vibrotactile transducers (tBCI). Experiment results of 13subjects performing online experiments validated the novel BCI paradigm. In the case of the autdBCI, only a single user’s results were bordering on the level of chance, and four subjects attained 100% (10 trials averaging). This project was a result of a collaboration of two laboratories [18], [19] allowing to train students in a novel contactless tactile modality and BCI online application development. V. CONCLUSIONS The reviewed successful student projects in this paper conﬁrmed a hypothesis of the computational neuroscience (neurotechnology) topics teaching suitability for the com- puter science students based on the project–based educa- tion principles realized in the BCI–LAB by introducing3669 Authorized licensed use limited to: Universidad Nacional de Colombia (UNAL). Downloaded on July 23,2024 at 22:20:41 UTC from IEEE Xplore.  Restrictions apply.
Fig. 4. A photograph of the AUTD BCI Paradigm [18], [19] project realized in the BCI–LAB. The user in the photograph manages a small LEGO robot based on intentional responses to the contactless tactile stimuli delivered thorough ultrasonic beam generated by the AUTD. An online video from the experiment is available at [28]. the modern signal processing, machine learning, multimedia (multi–sensory stimulation) and neuroscience topics in a single approach to the new BCI experimental paradigms developments. The future educational concepts development shall include interaction with real patients in need since so far the pre- sented BCI–LAB projects were tested with healthy users. REFERENCES [1] J. Wolpaw and E. W. Wolpaw, Eds., Brain-Computer Interfaces: Principles and Practice . Oxford University Press, 2012. [2] J. R. Patterson and M. Grabois, “Locked-in syndrome: a review of 139 cases.” Stroke , vol. 17, no. 4, pp. 758–64, 1986. [3] N. Nishikawa, S. Makino, and T. M. Rutkowski, “Spatial auditory bci paradigm based on real and virtual sound image generation,” in Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2013 Asia-Paciﬁc , 2013, pp. 1–5, paper ID 387. [4] M. Chang, N. Nishikawa, Z. R. Struzik, K. Mori, S. Makino, D. Mandic, and T. M. Rutkowski, “Comparison of P300 responses in auditory, visual and audiovisual spatial speller BCI paradigms,” inProceedings of the Fifth International Brain-Computer Interface Meeting 2013 . Asilomar Conference Center, Paciﬁc Grove, CA USA: Graz University of Technology Publishing House, Austria, June 3-7, 2013, p. Article ID: 156. [5] Y . Lelievre and T. M. Rutkowski, “Novel virtual moving sound- based spatial auditory brain-computer interface paradigm,” in Neural Engineering (NER), 2013 6th International IEEE/EMBS Conference on. IEEE Engineering in Medicine and Biology Society, 2013, pp. 9–12, arXiv:1308.2630 http://arxiv.org/abs/1308.2630. [6] B. Hieronymus, H. Mori, and T. M. Rutkowski,
arXiv:1308.2630 http://arxiv.org/abs/1308.2630. [6] B. Hieronymus, H. Mori, and T. M. Rutkowski, “Brain-computer interface using ambisonics-reproduced dynamic sound image effects,” inSoft Computing and Intelligent Systems (SCIS), 2014 Joint 7th International Conference on and Advanced Intelligent Systems (ISIS), 15th International Symposium on , December 2014, pp. 301–306. [7] M. Chang, K. Mori, S. Makino, and T. M. Rutkowski, “Spatial auditory two-step input Japanese syllabary brain-computer interface speller,” Procedia Technology , vol. 18, pp. 25 – 31, 2014. [8] M. Chang and T. M. Rutkowski, “Two–step input japanese kana characters spatial auditory brain–computer interface,” in Advances in Cognitive Neurodynamics Volume (V) . Springer Netherlands, 2015, pp. (accepted, in press). [9] C. Nakaizumi, T. Matsui, K. Mori, S. Makino, and T. M. Rutkowski, “Spatial auditory brain-computer interface using head related impulse response,” in Proceedings of The 10th AEARU Workshop on Computer Science and Web Technologies (CSWT-2015) . University of Tsukuba, February 2015, pp. 37–38. [10] ——, “Head–related impulse response-based spatial auditory brain– computer interface,” in Proceedings of the 6th International Brain- Computer Interface Conference 2014 , G. Mueller-Putz, G. Bauern- feind, C. Brunner, D. Steyrl, S. Wriessnegger, and R. Scherer, Eds. Graz University of Technology Publishing House, 2014, pp. Article ID 020–1–4.[11] D. Aminaka, S. Makino, and T. M. Rutkowski, “SSVEP brain- computer interface using green and blue lights,” in Proceedings of The 10th AEARU Workshop on Computer Science and Web Technologies (CSWT-2015) . University of Tsukuba, February 2015, pp. 39–40. [12] ——, “Chromatic SSVEP BCI paradigm targeting the higher fre- quency eeg responses,” in Asia-Paciﬁc Signal and Information Pro- cessing Association, 2014 Annual Summit and Conference (APSIPA) , December 2014, pp. 1–7. [13] T. Kodama, S. Makino, and T. M. Rutkowski, “Spatial tactile brain- computer
2014, pp. 1–7. [13] T. Kodama, S. Makino, and T. M. Rutkowski, “Spatial tactile brain- computer interface paradigm applying vibration stimuli to large ar- eas of user’s back,” in Proceedings of the 6th International Brain- Computer Interface Conference 2014 , G. Mueller-Putz, G. Bauern- feind, C. Brunner, D. Steyrl, S. Wriessnegger, and R. Scherer, Eds. Graz University of Technology Publishing House, 2014, pp. Article ID 032–1–4. [14] H. Yajima, S. Makino, and T. M. Rutkowski, “P300 responses classi- ﬁcation improvement in tactile bci with touch-sense glove,” in Asia- Paciﬁc Signal and Information Processing Association, 2014 Annual Summit and Conference (APSIPA) . IEEE, December 2014, pp. 1–7. [15] T. M. Rutkowski and H. Mori, “Tactile and bone-conduction auditory brain computer interface for vision and hearing impaired users,” Journal of Neuroscience Methods , vol. 244, no. 0, pp. 45 – 51, 2015, brain Computer Interfaces; Tribute to Greg A. Gerhardt. [16] S. Kono and T. M. Rutkowski, “Tactile-force brain-computer interface paradigm,” Multimedia Tools and Applications , vol. ’Online First’ on SpringerLink, pp. 1–13, December 2014. [17] K. Shimizu, H. Mori, S. Makino, and T. M. Rutkowski, “Tactile pres- sure brain-computer interface using point matrix pattern paradigm,” inSoft Computing and Intelligent Systems (SCIS), 2014 Joint 7th International Conference on and Advanced Intelligent Systems (ISIS), 15th International Symposium on , December 2014, pp. 473–477. [18] T. M. Rutkowski and H. Shinoda, “Airborne ultrasonic tactile display contactless brain-computer interface paradigm,” in 2015 International Workshop on Clinical Brain-Machine Interface Systems (CBMI 2015) , Tokyo, Japan, March 13–15, 2015, p. 39. [19] K. Hamada, H. Mori, H. Shinoda, and T. M. Rutkowski, “Airborne ultrasonic tactile display brain-computer interface paradigm,” in Pro- ceedings of the 6th International Brain-Computer Interface Confer- ence 2014 , G. Mueller-Putz, G. Bauernfeind, C. Brunner,
Brain-Computer Interface Confer- ence 2014 , G. Mueller-Putz, G. Bauernfeind, C. Brunner, D. Steyrl, S. Wriessnegger, and R. Scherer, Eds. Graz University of Technology Publishing House, 2014, pp. Article ID 018–1–4. [20] Y . Matsumoto, N. Nishikawa, S. Makino, T. Yamada, and T. Rutkowski, “Auditory steady-state response stimuli based BCI application - the optimization of the stimuli types and lengths,” inSignal Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Paciﬁc , 2012, pp. 1–7, paper ID 285. [21] Y . Matsumoto, S. Makino, K. Mori, and T. M. Rutkowski, “Classi- fying P300 responses to vowel stimuli for auditory brain-computer interface,” in Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2013 Asia-Paciﬁc , 2013, pp. 1–5, paper ID 388. [22] H. Mori, Y . Matsumoto, Z. R. Struzik, K. Mori, S. Makino, D. Mandic, and T. M. Rutkowski, “Multi-command tactile and auditory brain computer interface based on head position stimulation,” in Proceedings of the Fifth International Brain-Computer Interface Meeting 2013 . Asilomar Conference Center, Paciﬁc Grove, CA USA: Graz University of Technology Publishing House, Austria, June 3-7, 2013, p. Article ID: 095. [23] G. Schalk and J. Mellinger, A Practical Guide to Brain–Computer Interfacing with BCI2000 . Springer-Verlag London Limited, 2010. [24] Y . Renard, F. Lotte, G. Gibert, M. Congedo, E. Maby, V . Delannoy, O. Bertrand, and A. L ´ecuyer, “Openvibe: an open-source software platform to design, test, and use brain-computer interfaces in real and virtual environments,” Presence: teleoperators and virtual envi- ronments , vol. 19, no. 1, pp. 35–53, 2010. [25] “Max 6,” 2012. [Online]. Available: http://cycling74.com/ [26] T. M. Rutkowski, “Japanese kana character spatial auditory BCI speller.” [Online]. Available: http://youtu.be/BY1kSqh1NEY [27] ——, “Chromatic SSVEP BCI-based NAO robot control.” [Online]. Available: http://youtu.be/gkzkJ7VM5ps [28]
SSVEP BCI-based NAO robot control.” [Online]. Available: http://youtu.be/gkzkJ7VM5ps [28] ——, “The autdBCI and a robot control (the winner project of The BCI Annual Research Award 2014),” YouTube video. [Online]. Available: http://youtu.be/JE29CMluBh03670 Authorized licensed use limited to: Universidad Nacional de Colombia (UNAL). Downloaded on July 23,2024 at 22:20:41 UTC from IEEE Xplore.  Restrictions apply.
Student Teaching and Research Laboratory Focusing on Brain–computer Interface Paradigms – A Creative Environment for Computer Science Students – Tomasz M. Rutkowski1,2 Abstract — This paper presents an applied concept of a brain– computer interface (BCI) student research laboratory (BCI– LAB) at the Life Science Center of TARA, University of Tsukuba, Japan. Several successful case studies of the student projects are reviewed together with the BCI Research Award 2014 winner case. The BCI–LAB design and project–based teaching philosophy is also explained. Future teaching and research directions summarize the review. I. INTRODUCTION Brain computer interface (BCI) is a technology that uses brain neural activities only to control a computer, or a machine, without any body muscle movements [1]. This technology could allow disabled people, such as locked–in syndrome (LIS) suffering patients [2], to regain communica- tion skills or to manage daily life support related functions. On the other hand, the BCI technology creates a perfect platform for computer science students to study computa- tional neuroscience–based neurotechnology methods. There are also many types of BCIs based on the utilized brainwave signals and experimental paradigms. This allows to create the multi–sensory experimental labs with biomedical (e.g. neu- rophysiological) signals capturing and processing in mind. Creation of multi–sensory stimuli allows also the students to practically develop and test multimedia environments for a future deployment of the developed paradigms for paralyzed patients. This review paper presents a successful implementation of the BCI teaching and experimental lab at the Life Science Center of TARA, University of Tsukuba, Japan. The experimental focus of the non–invasive BCI lab is on the external stimulus–driven paradigms relaying on event related potential’s (ERP) P300 responses [1] allowing for multi–sensory stimulation application. The sensory modali- ties available for
[1] allowing for multi–sensory stimulation application. The sensory modali- ties available for experimenting are as follows, (i) spatial au- ditory: with multi–loudspeaker studio conﬁgurations [3]–[8] and headphone viral sound–based [9], [10]; (ii) spatial visual: P300 [4] and steady–state response–based [11], [12]; (iii) spatial tactile: realized with vibration–based P300 [13]–[15], tactile–force [16], pin–pressure [17], and airborne ultrasonic ∗The presented student research and educational laboratory was supported in part by the Strategic Information and Communications R&D Promotion Program (SCOPE) no. 121803027 of The Ministry of Internal Affairs and Communication in Japan, YAMAHA Corporation and by KAKENHI, the Japan Society for the Promotion of Science, grant no. 24243062 . 1Tomasz M. Rutkowski is with Life Science Center of TARA, Uni- versity of Tsukuba, 1-1-1 Tennodai, Tsukuba, Ibaraki, 305-8577 Japan, tomek@bci-lab.info http://bci-lab.info/ 2Tomasz M. Rutkowski is also with RIKEN Brain Science Institute, Wako-shi, Saitama, Japantactile display (AUTD) contactless somatosensory modal- ity [18], [19]. Non–invasive BCIs are the most safe because the brainwave sensors are usually attached to the surface of a human head, thus such experiments, within the institu- tional ethical committee guidelines, are perfectly suitable for computer science students. All the experiments reviewed and reported in this paper were conducted in agreement with the ethical committee guidelines of the Faculty of Engineering, Information and Systems at University of Tsukuba, Tsukuba, Japan (experimental permission no. 2013R7). The non– invasive BCI modalities, however, suffer usually from lower brainwave quality due to a very poor signal to noise ratio comparing to the invasive systems, which creates a perfect educational project platform for the engineering students, who design ﬁltering and feature extraction algorithms to cope with the above problems [15], [20], [21]. The
ﬁltering and feature extraction algorithms to cope with the above problems [15], [20], [21]. The non–invasive BCIs are divided into stimulus–driven. The stimulus–driven BCIs constitute the interactive interfacing solutions in which the commands to operate a computer or an application are associated with sensory stimuli delivered to the user. The user generates a signature brainwave captured in EEG by focusing attention (on the so–called target eliciting the P300) or by ignoring (the non–target ) the randomly presented sensory auditory, tactile or visual stimuli. The students of the BCI– LAB design those spatial sensory stimulations and enhance practically their multimedia knowledge with support of the neurotechnology (brain response–driven) applications. Among the recently developed BCIs the visual modal- ity [1] is currently the most successful because the evoked P300 responses have usually the largest amplitudes allowing for the most easy classiﬁcation [4]. However, it is known that LIS users (e.g. advanced stage amyotrophic lateral sclerosis patients) cannot use visual modality because they gradually lose intentional muscle control, including eye movements, focusing or intentional blinking. Our research hypothesis is that the auditory and tactile modalities BCI shall be a more suitable solution for communication establishment for LIS users. The students of the BCI–LAB have an opportunity to design “novel sensory stimulations” by combining tactile and bone–conduction–auditory [22]; audiovisual [4]; and contactless tactile [18], [19] paradigms. The rest of the paper is organized as follows. In the next section we describe the teaching philosophy behind the BCI–LAB concept. Next the experimental environment is described focusing on the hardware, software and stimulus generators. Finally the already published student projects978-1-4244-9270-1/15/$31.00 ©2015 IEEE3667 Authorized licensed use limited to: Universidad Nacional de Colombia (UNAL). Downloaded on July
Authorized licensed use limited to: Universidad Nacional de Colombia (UNAL). Downloaded on July 23,2024 at 22:20:41 UTC from IEEE Xplore.  Restrictions apply.
are brieﬂy described. Finally, the conclusions are drawn and future research and educational directions outlined. II. TEACHING PHILOSOPHY A teaching philosophy implemented in the BCI–LAB is based on illustrating the beauty of a neuroscience-driven creative innovation of media and human augmentation. Even though computational neuroscience and the modern media or human augmentation might be regarded as not very closely related research disciplines in the eyes of the general public, practical approaches often result with the both disciplines sharing outcomes from the both ﬁelds. For most the BCI– LAB students the projects are their ﬁrst chance to study brain-related subjects in application to real-life multimedia and human augmentation applications’ development. To be- gin each new project, we use in the lab an analogy to real life examples to lead the students to the reasonability of the method. It happens that when we talk about the vision, hearing or some other perceptual methods we are ourselves so familiar with, we ﬁnd ourselves facing a room of confused faces. The best remedy for such initial confusion is a participation in as experimental subjects in ongoing lab projects usually run by the senior students. A master–student relationship between students could be simply realized, which helps with lab culture establishment. A base for an successful experimental lab foundation is an interactive and student–friendly teaching approach. No lab seminar notes can be recycled from one semester to the next, nor is it best to use the same examples. Students are different and their ways of developing understanding too. Our lab philosophy is that a senior experimenter should always be ready to adjust an advisory strategy due to the needs of the new students. If any new to the project student complains about the problem or a lack of available solution, a drill experimental example may become necessary; if there is one tricky problem in the experimental setting, a little hint
may become necessary; if there is one tricky problem in the experimental setting, a little hint given will make the students efforts more inspired; if the experimental design has a dull nature, adding some interesting trial examples may lighten it up and make the lab life more enjoyable and less slumberous. The concept of our BCI–LAB is to train the computer science students in computational neuroscience problems by using ”hands–on” practical problems’ learning philosophy. III. BCI–LAB ENVIRONMENT There are three pillars of a successful experimental stimulus–driven BCI–LAB, namely a good experimental hardware environment; ﬂexible software platforms for brain- wave signal processing and classiﬁcation; an excellent multi– sensory stimulus design platform. The experimental hardware, allowing for EEG capture for the online BCI experiments discussed in the following sec- tions, has been set up based on g.USBamp and g.MOBIlab+ from g.tec Medical Engineering, Austria (used in the major- ity of the projects in BCI–LAB), as well as on vAmp from Brain Products GmbH, Germany (used only in [7], [8]). All the systems used the active EEG electrodes g.LADYbird and g.SAHARA from g.tec Medical Engineering, Austria. Fig. 1. A photograph of a two–step input Japanese kana characters spatial auditory BCI [7], [8] project realized in the BCI–LAB. The user in the photograph sits in front of ten loudspeakers (sponsored by YAMAHA Corporation) delivering the spatial sound images. The yellow Japanese “na” character visualizes (not seen to the user) the sound image location. An online video from the experiment is available at [26]. Due to different EEG ampliﬁers manufacturers we decided to use, and to extend with own functions, ﬂexible signal acquisition and classiﬁcation environments BCI2000 [23] and OpenVibe [24]. The both environments allow for ﬂexible extensions also developed and applied by our team [11], [21]. Finally the spatial auditory, visual and tactile stimulation generators were
our team [11], [21]. Finally the spatial auditory, visual and tactile stimulation generators were programmed by our team in MAX 6 [25] and delivered to the users via multichannel sound cards, large computer displays or tactile transducers managed by ARDUINO micro–controller boards. IV. BCI–LAB STUDENT PROJECTS 1) Multi–loudspeaker–based Spatial Auditory BCIs: Three different multi–loudspeaker–based spatial auditory BCI paradigms were realized in the BCI–LAB allowing the students to design and test multi–source auditory environ- ments using MAX 6 visual programming environment [25]. The ﬁrst approach implemented a vector–based amplitude panning method to spatial horizontal sound images posi- tioned octagonally around the user’s head [3]. The second approach involved moving sound stimuli realized in half– spherical loudspeaker environments [5], [6]. The third, and the most successful application, realized a two–step–input speller based on the spatial auditory sound images pre- sentation [7], [8] as shown in Figure 1. The application has been developed for 45Japanese syllabary ( kana ) letters using a two–step–input procedure in order to create an easy to use BCI–speller interface. The results of the con- ducted online EEG experiments have shown the feasibility of the Japanese kana characters set in two–step–input saBCI– speller paradigm. Also we conﬁrmed that the accuracy of each modality had no signiﬁcant differences. A video doc- umenting the online saBCI–speller experiment is available at [26]. The three presented auditory BCI paradigms allowed for training of the computer science students in spatial sound design and biomedical signal acquisition (EEG), processing and classiﬁcation for the online applications. 2) Headphone–based Spatial Auditory BCI Paradigm: This study reported on a head related impulse response (HRIR) application to an auditory spatial brain-computer interface (BCI) speller paradigm [9], [10]. Six experienced and ﬁve BCI–naive users participated
interface (BCI) speller paradigm [9], [10]. Six experienced and ﬁve BCI–naive users participated in an experimental3668 Authorized licensed use limited to: Universidad Nacional de Colombia (UNAL). Downloaded on July 23,2024 at 22:20:41 UTC from IEEE Xplore.  Restrictions apply.
Fig. 2. A photograph of a chromatic SSVEP BCI project [11], [12] realized in the BCI–LAB. The user in the photograph sits in front of a frame with four green–blue LEDs. A humanoid robot is managed using four commands from the SSVEP BCI. An online video from the experiment is available at [27]. spelling set up based on ﬁve Japanese vowels. Obtained auditory evoked potentials resulted with encouragingly good and stable P300–responses in online BCI experiments. Our case study indicated that the headphone reproduced auditory (HRIR–based) spatial sound paradigm could be a viable alternative to the established multi–loudspeaker surround sound BCI–speller applications, as far as healthy pilot study users are concerned. This project allowed the students to practically apply headphone–based spatial sound virtualiza- tion to the online auditory BCI application [9], [10]. 3) Visual SSVEP and cVEP Paradigms: A novel approach to steady–state visual evoked potentials (SSVEP) based BCI was developed [11], [12] with further extension to a code– modulated visual evoked potential (cVEP) approach. To minimize possible side effects of the monochromatic light SSVEP we proposed to utilize color information in form of the green and blue interlaced ﬂicker (see Figure 2 with a robot control application). The feasibility of the proposed method was evaluated in a comparison of the classical monochromatic versus the proposed colorful SSVEP re- sponses. The proposed method scored with higher accuracies comparing to the monochromatic SSVEP [11]. This visual modality project allowed the student to attack the classical problems related to ﬂickering light–based stimuli (danger of photosensitive epilepsy, etc.). As result two solutions were proposed based on higher frequency and cVEP–based stimuli together with less sensitive chromatic light modality. 4) Bone–conduction Auditory and Tactile BCI Paradigm: In this project we studied the extent to which vibrotactile stimuli delivered to the head of a
In this project we studied the extent to which vibrotactile stimuli delivered to the head of a subject could serve as a platform for the BCI paradigm [22] (see Figure 3). Six head positions were used to evoke combined somatosensory and auditory (via bone-conduction effect) brain responses, in order to deﬁne a multimodal tactile and auditory brain computer interface (taBCI). Experimental results on subjects performing online taBCI, using stimuli with a moderately fast inter-stimulus-interval, validated the taBCI paradigm, while the feasibility of the concept was illuminated through encouraﬁng information-transfer rate case studies. In the reported experiments [22] only a single BCI-nave subject obtained 100% and also one obtained 0%for the six digit sequence spelling accuracy with 5-trials averaging procedure. Fig. 3. A photograph of a bone–conduction auditory and tactile multi– sensory BCI Paradigm [15], [22] project realized in the BCI–LAB. The user in the photograph manages a walking virtual avatar based on intentional responses to the combined tactile and bone–conduction auditory stimuli delivered thorough vibrotactile transducers embedded in the EEG cap (large metallic discs above the ear and on a forehead in the picture). The results were further improved with an implementation of a novel synchro–squeezing EEG preprocessing method as reported in [15]. This project allowed the BCI–LAB students for a creation of a novel multi–sensory BCI platform [22] together with the new signal processing method develop- ment [15]. 5) Vibrotactile and Tactile–pressure BCI Paradigms: A series of successful vibrotactile [13], [14] and tactile– pressure/force [16], [17] paradigms were developed with the BCI–LAB approaching a problem of communication alternatives design for those LIS patients with no vision or functional hearing (due to the so called ear stacking syndrome). In these spatial tactile projects the contributing students combined successfully their knowledge of multi-
spatial tactile projects the contributing students combined successfully their knowledge of multi- channel stimulus generation systems development in MAX 6 environment [25] together with brainwave processing and classiﬁcation. 6) Airborne Ultrasonic Tactile Display BCI Paradigm: In this BCI Research Award 2014 winning project a contact– less somatosensory stimuli were delivered via an airborne ultrasonic tactile display (AUTD) to the palms of a user (see Figure 4) and served as a platform for the BCI paradigm (autdBCI) [18], [19]. Six palm positions were used to evoke combined somatosensory brain responses, in order to deﬁne a novel contact-less tactile BCI. A comparison was made with classical attached vibrotactile transducers (tBCI). Experiment results of 13subjects performing online experiments validated the novel BCI paradigm. In the case of the autdBCI, only a single user’s results were bordering on the level of chance, and four subjects attained 100% (10 trials averaging). This project was a result of a collaboration of two laboratories [18], [19] allowing to train students in a novel contactless tactile modality and BCI online application development. V. CONCLUSIONS The reviewed successful student projects in this paper conﬁrmed a hypothesis of the computational neuroscience (neurotechnology) topics teaching suitability for the com- puter science students based on the project–based educa- tion principles realized in the BCI–LAB by introducing3669 Authorized licensed use limited to: Universidad Nacional de Colombia (UNAL). Downloaded on July 23,2024 at 22:20:41 UTC from IEEE Xplore.  Restrictions apply.
Fig. 4. A photograph of the AUTD BCI Paradigm [18], [19] project realized in the BCI–LAB. The user in the photograph manages a small LEGO robot based on intentional responses to the contactless tactile stimuli delivered thorough ultrasonic beam generated by the AUTD. An online video from the experiment is available at [28]. the modern signal processing, machine learning, multimedia (multi–sensory stimulation) and neuroscience topics in a single approach to the new BCI experimental paradigms developments. The future educational concepts development shall include interaction with real patients in need since so far the pre- sented BCI–LAB projects were tested with healthy users. REFERENCES [1] J. Wolpaw and E. W. Wolpaw, Eds., Brain-Computer Interfaces: Principles and Practice . Oxford University Press, 2012. [2] J. R. Patterson and M. Grabois, “Locked-in syndrome: a review of 139 cases.” Stroke , vol. 17, no. 4, pp. 758–64, 1986. [3] N. Nishikawa, S. Makino, and T. M. Rutkowski, “Spatial auditory bci paradigm based on real and virtual sound image generation,” in Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2013 Asia-Paciﬁc , 2013, pp. 1–5, paper ID 387. [4] M. Chang, N. Nishikawa, Z. R. Struzik, K. Mori, S. Makino, D. Mandic, and T. M. Rutkowski, “Comparison of P300 responses in auditory, visual and audiovisual spatial speller BCI paradigms,” inProceedings of the Fifth International Brain-Computer Interface Meeting 2013 . Asilomar Conference Center, Paciﬁc Grove, CA USA: Graz University of Technology Publishing House, Austria, June 3-7, 2013, p. Article ID: 156. [5] Y . Lelievre and T. M. Rutkowski, “Novel virtual moving sound- based spatial auditory brain-computer interface paradigm,” in Neural Engineering (NER), 2013 6th International IEEE/EMBS Conference on. IEEE Engineering in Medicine and Biology Society, 2013, pp. 9–12, arXiv:1308.2630 http://arxiv.org/abs/1308.2630. [6] B. Hieronymus, H. Mori, and T. M. Rutkowski,
arXiv:1308.2630 http://arxiv.org/abs/1308.2630. [6] B. Hieronymus, H. Mori, and T. M. Rutkowski, “Brain-computer interface using ambisonics-reproduced dynamic sound image effects,” inSoft Computing and Intelligent Systems (SCIS), 2014 Joint 7th International Conference on and Advanced Intelligent Systems (ISIS), 15th International Symposium on , December 2014, pp. 301–306. [7] M. Chang, K. Mori, S. Makino, and T. M. Rutkowski, “Spatial auditory two-step input Japanese syllabary brain-computer interface speller,” Procedia Technology , vol. 18, pp. 25 – 31, 2014. [8] M. Chang and T. M. Rutkowski, “Two–step input japanese kana characters spatial auditory brain–computer interface,” in Advances in Cognitive Neurodynamics Volume (V) . Springer Netherlands, 2015, pp. (accepted, in press). [9] C. Nakaizumi, T. Matsui, K. Mori, S. Makino, and T. M. Rutkowski, “Spatial auditory brain-computer interface using head related impulse response,” in Proceedings of The 10th AEARU Workshop on Computer Science and Web Technologies (CSWT-2015) . University of Tsukuba, February 2015, pp. 37–38. [10] ——, “Head–related impulse response-based spatial auditory brain– computer interface,” in Proceedings of the 6th International Brain- Computer Interface Conference 2014 , G. Mueller-Putz, G. Bauern- feind, C. Brunner, D. Steyrl, S. Wriessnegger, and R. Scherer, Eds. Graz University of Technology Publishing House, 2014, pp. Article ID 020–1–4.[11] D. Aminaka, S. Makino, and T. M. Rutkowski, “SSVEP brain- computer interface using green and blue lights,” in Proceedings of The 10th AEARU Workshop on Computer Science and Web Technologies (CSWT-2015) . University of Tsukuba, February 2015, pp. 39–40. [12] ——, “Chromatic SSVEP BCI paradigm targeting the higher fre- quency eeg responses,” in Asia-Paciﬁc Signal and Information Pro- cessing Association, 2014 Annual Summit and Conference (APSIPA) , December 2014, pp. 1–7. [13] T. Kodama, S. Makino, and T. M. Rutkowski, “Spatial tactile brain- computer --- Brain-Computer Interface Confer- ence 2014 , G. Mueller-Putz, G. Bauernfeind, C. Brunner, D. Steyrl, S. Wriessnegger, and R. Scherer, Eds. Graz University of Technology Publishing House, 2014, pp. Article ID 018–1–4. [20] Y . Matsumoto, N. Nishikawa, S. Makino, T. Yamada, and T. Rutkowski, “Auditory steady-state response stimuli based BCI application - the optimization of the stimuli types and lengths,” inSignal Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Paciﬁc , 2012, pp. 1–7, paper ID 285. [21] Y . Matsumoto, S. Makino, K. Mori, and T. M. Rutkowski, “Classi- fying P300 responses to vowel stimuli for auditory brain-computer interface,” in Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2013 Asia-Paciﬁc , 2013, pp. 1–5, paper ID 388. [22] H. Mori, Y . Matsumoto, Z. R. Struzik, K. Mori, S. Makino, D. Mandic, and T. M. Rutkowski, “Multi-command tactile and auditory brain computer interface based on head position stimulation,” in Proceedings of the Fifth International Brain-Computer Interface Meeting 2013 . Asilomar Conference Center, Paciﬁc Grove, CA USA: Graz University of Technology Publishing House, Austria, June 3-7, 2013, p. Article ID: 095. [23] G. Schalk and J. Mellinger, A Practical Guide to Brain–Computer Interfacing with BCI2000 . Springer-Verlag London Limited, 2010. [24] Y . Renard, F. Lotte, G. Gibert, M. Congedo, E. Maby, V . Delannoy, O. Bertrand, and A. L ´ecuyer, “Openvibe: an open-source software platform to design, test, and use brain-computer interfaces in real and virtual environments,” Presence: teleoperators and virtual envi- ronments , vol. 19, no. 1, pp. 35–53, 2010. [25] “Max 6,” 2012. [Online]. Available: http://cycling74.com/ [26] T. M. Rutkowski, “Japanese kana character spatial auditory BCI speller.” [Online]. Available: http://youtu.be/BY1kSqh1NEY [27] ——, “Chromatic SSVEP BCI-based NAO robot control.” [Online]. Available: http://youtu.be/gkzkJ7VM5ps [28]
2014, pp. 1–7. [13] T. Kodama, S. Makino, and T. M. Rutkowski, “Spatial tactile brain- computer interface paradigm applying vibration stimuli to large ar- eas of user’s back,” in Proceedings of the 6th International Brain- Computer Interface Conference 2014 , G. Mueller-Putz, G. Bauern- feind, C. Brunner, D. Steyrl, S. Wriessnegger, and R. Scherer, Eds. Graz University of Technology Publishing House, 2014, pp. Article ID 032–1–4. [14] H. Yajima, S. Makino, and T. M. Rutkowski, “P300 responses classi- ﬁcation improvement in tactile bci with touch-sense glove,” in Asia- Paciﬁc Signal and Information Processing Association, 2014 Annual Summit and Conference (APSIPA) . IEEE, December 2014, pp. 1–7. [15] T. M. Rutkowski and H. Mori, “Tactile and bone-conduction auditory brain computer interface for vision and hearing impaired users,” Journal of Neuroscience Methods , vol. 244, no. 0, pp. 45 – 51, 2015, brain Computer Interfaces; Tribute to Greg A. Gerhardt. [16] S. Kono and T. M. Rutkowski, “Tactile-force brain-computer interface paradigm,” Multimedia Tools and Applications , vol. ’Online First’ on SpringerLink, pp. 1–13, December 2014. [17] K. Shimizu, H. Mori, S. Makino, and T. M. Rutkowski, “Tactile pres- sure brain-computer interface using point matrix pattern paradigm,” inSoft Computing and Intelligent Systems (SCIS), 2014 Joint 7th International Conference on and Advanced Intelligent Systems (ISIS), 15th International Symposium on , December 2014, pp. 473–477. [18] T. M. Rutkowski and H. Shinoda, “Airborne ultrasonic tactile display contactless brain-computer interface paradigm,” in 2015 International Workshop on Clinical Brain-Machine Interface Systems (CBMI 2015) , Tokyo, Japan, March 13–15, 2015, p. 39. [19] K. Hamada, H. Mori, H. Shinoda, and T. M. Rutkowski, “Airborne ultrasonic tactile display brain-computer interface paradigm,” in Pro- ceedings of the 6th International Brain-Computer Interface Confer- ence 2014 , G. Mueller-Putz, G. Bauernfeind, C. Brunner,
SSVEP BCI-based NAO robot control.” [Online]. Available: http://youtu.be/gkzkJ7VM5ps [28] ——, “The autdBCI and a robot control (the winner project of The BCI Annual Research Award 2014),” YouTube video. [Online]. Available: http://youtu.be/JE29CMluBh03670 Authorized licensed use limited to: Universidad Nacional de Colombia (UNAL). Downloaded on July 23,2024 at 22:20:41 UTC from IEEE Xplore.  Restrictions apply.
El texto discute la implementación de un laboratorio de investigación estudiantil de interfaz cerebro-computadora (BCI-LAB) en el Centro de Ciencias de la Vida de TARA, Universidad de Tsukuba, Japón. Se destacan varios proyectos exitosos de estudiantes y el ganador del Premio de Investigación BCI 2014. Se explica la filosofía de diseño y enseñanza basada en proyectos del BCI-LAB, resaltando el potencial para que los estudiantes de informática estudien métodos de neurotecnología basados en neurociencia computacional. También se discuten las aplicaciones de la tecnología para personas discapacitadas, como pacientes con síndrome de enclaustramiento.

El texto detalla los diferentes tipos de BCIs basados en señales cerebrales y paradigmas experimentales, permitiendo la creación de laboratorios experimentales multi-sensoriales con capacidades de captura y procesamiento de señales biomédicas. Esto permite a los estudiantes desarrollar y probar entornos multimedia para su posible uso en asistir a pacientes paralizados. El enfoque del laboratorio de BCI no invasivo se centra en paradigmas impulsados por estímulos externos que se basan en las respuestas P300 de potenciales relacionados con eventos (ERP), facilitando aplicaciones de estimulación multi-sensorial.

En general, el texto destaca la exitosa implementación del laboratorio de enseñanza y experimentación de BCI en la Universidad de Tsukuba, Japón, y discute las futuras direcciones para la enseñanza e investigación en el campo de las interfaces cerebro-computadora. Se mencionan las modalidades sensoriales disponibles para experimentar, como auditiva espacial, visual espacial y táctil espacial, y se describen configuraciones de estudio con múltiples altavoces, sonido basado en auriculares, respuestas basadas en P300 y vibración basada en P300, entre otros.

Además, se discute la importancia de la ética en la investigación y se menciona que todos los experimentos se llevaron a cabo siguiendo las pautas éticas de la Universidad de Tsukuba. Se destaca la seguridad de las interfaces cerebro-computadora no invasivas y se menciona el apoyo financiero recibido para el laboratorio de investigación estudiantil. Se describen varios proyectos estudiantiles realizados en el BCI-LAB, incluyendo paradigmas auditivos y táctiles, así como el uso de tecnología de ultrasonido aéreo para estimulación táctil sin contacto.

En resumen, el texto detalla la filosofía de enseñanza y experimentación del BCI-LAB, resaltando la importancia de la educación basada en proyectos y la colaboración entre estudiantes y experimentadores senior. Se mencionan varios proyectos exitosos realizados en el laboratorio, que abordan diferentes modalidades sensoriales y aplicaciones de la tecnología BCI para asistir a personas con discapacidades. Se destaca la importancia de crear un entorno estimulante para los estudiantes de informática y se mencionan futuras direcciones para la investigación en el campo de las interfaces cerebro-computadora.
