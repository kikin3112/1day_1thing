{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "32210\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que haya PDFs en la carpeta 'docs'\n",
    "documents = DirectoryLoader('./docs/', glob=\"./*.pdf\", loader_cls=PyPDFLoader).load()\n",
    "# Tratameinto de caracteres indeseados\n",
    "for d in documents:\n",
    "    d.page_content = d.page_content.replace('\\n', '- ').replace('\\t', '- ')\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "texts = [doc.page_content for doc in docs]\n",
    "\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n --- \\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "\n",
    "print(type(concatenated_content))\n",
    "print(len(concatenated_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/221328895- Comparing Hu mans and AI Agents- Conf erence Paper  · August 2011- DOI: 10.1007/978-3-642-22887-2_13  · Sour ce: DBLP- CITATIONS- 50READS- 963- 5 author s, including:- Sergio Esp aña- Utrecht Univ ersity- 114 PUBLICA TIONS    1,187  CITATIONS    - SEE PROFILE- Victoria Hernánde z-Llor eda- Complut ense Univ ersity of Madrid- 34 PUBLICA TIONS    1,956  CITATIONS    - SEE PROFILE- Jose Hernande z-Orallo- Univ ersitat P olitècnic a de V alència- 255 PUBLICA TIONS    5,290  CITATIONS    - SEE PROFILE- All c ontent f ollo wing this p age was uplo aded b y Sergio Esp aña  on 02 June 2014.- The user has r equest ed enhanc ement of the do wnlo aded file.' metadata={'source': 'docs\\\\test.pdf', 'page': 0}\n",
      "page_content='Comparing humans and AI agents- Javier Insa-Cabrera1David L. Dowe2Sergio Espa˜ na-Cubillo3- M.Victoria Hern´ andez-Lloreda4Jos´ e Hern´ andez-Orallo1- 1DSIC, Universitat Polit` ecnica de Val` encia, Spain. {jinsa, jorallo}@dsic.upv.es- 2Clayton School of Information Technology, Monash University, Australia.- david.dowe@monash.edu- 3ProS Research Center, Universitat Polit` ecnica de Val` encia, Spain.- sergio.espana@pros.upv.es- 4Departamento de Metodolog´ ıa de las Ciencias del Comportamiento, Universidad- Complutense de Madrid, Spain. vhlloreda@psi.ucm.es- Abstract. Comparing humans and machines is one important source of- information about both machine and human strengths and limitations.- Most of these comparisons and competitions are performed in rather- speciﬁc tasks such as calculus, speech recognition, translation, games,- etc. The information conveyed by these experiments is limited, since it- portrays that machines are much better than humans at some domains- and worse at others. In fact, CAPTCHAs exploit this fact. However,- there have only been a few proposals of general intelligence tests in the- last two decades, and, to our knowledge, just a couple of implementations- and evaluations. In this paper, we implement one of the most recent test- proposals, devise an interface for humans and use it to compare the- intelligence of humans and Q-learning, a popular reinforcement learning- algorithm. The results are highly informative in many ways, raising many- questions on the use of a (universal) distribution of environments, on the- role of measuring knowledge acquisition, and other issues, such as speed,- duration of the test, scalability, etc.- Keywords: Intelligence measurement, universal intelligence, general vs.- speciﬁc intelligence, reinforcement learning, IQ tests.- 1 Introduction- It is well-known that IQ tests are not useful for evaluating the intelligence of- machines. The main reason is not because machines are not able to ‘understand’- the test. The real reason is scarcely known and poorly understood, since available- theories do not manage to fully explain the empirical observations: it has been- shown that relative simple programs can be designed to score well on these tests- [11]. Some other approaches such as the Turing Test [15] and Captchas [17] have- their niches, but they are also inappropriate to evaluate AGI systems.- In the last ﬁfteen years, several alternatives for a general (or universal) intel-- ligence test (or deﬁnition) based on Solomonoﬀ’s universal distributions [12] (or- related ideas such as MML, compression or Kolmogorov complexity) have been' metadata={'source': 'docs\\\\test.pdf', 'page': 1}\n",
      "page_content='appearing on the scene [1, 3, 7, 8, 5], claiming that they are able to deﬁne or eval-- uate (machine) intelligence. In this paper we use one of these tests, a prototype- based on the anytime intelligence test presented in [5] and the environment class- introduced in [4], to evaluate one easily accessible biological system ( Homo sapi-- ens) and one oﬀ-the-shelf AI system, a popular reinforcement algorithm known- as Q-learning [18]. In order to do the comparison we use the same environment- class for both types of systems and we design hopefully non-biased interfaces for- both. We perform a pilot experiment on a reduced group of individuals.- From this experiment we obtain a number of interesting ﬁndings and insights.- First, it is possible to do the same test for humans and machines without being- anthropomorphic. The test is exactly the same for both and it is founded on a- theory derived from sound computational concepts. We just adapt the interface- (what way rewards, actions and observations look like) depending on the type of- subjects. Second, humans are not better than Q-learning in this test, even though- the test (despite several simpliﬁcations) is based on a universal distribution of- environments over a very general environment class. Third, since these results- are consistent to those in [11] (which show that machines can score well in IQ- tests), this gives additional evidence that a test which is valid for humans or- for machines separately might be useless to distinguish or to place humans and- machines on the same scale, so failing to be a universal intelligence test.- The following section overviews the most important proposals on deﬁning- and measuring machine intelligence to date, and, from them, it describes the- intelligence test and the environment class we will use in this paper. Sections- 3 and 4 describe the testing setting, the two types of agents we evaluate (Q-- learning and humans) and their interfaces. Section 5 includes the comparison- of the experimental results, analysing them by several factors. Finally, section 6- examines these results in a deeper way and draws several conclusions about the- way universal intelligence tests should and should not be.- 2 Measuring intelligence universally- Measuring machine intelligence or, more generally, performance has been virtu-- ally relegated to a philosophical or, at most, theoretical issue in AI. Given that- state-of-the-art technology in AI is still far from truly intelligent machines, it- seems that the Turing Test [15] (and its many variations [10]) and Captchas- [17] are enough for philosophical debates and practical applications respectively.- There are also tests and competitions in restricted domains, such as competi-- tions in robotics, in game playing, in machine translation and in reinforcement- learning (RL), most notably the RL competition. All of them use a somewhat- arbitrary and frequently anthropomorphic set of tasks.- An alternative, general proposal for intelligence and performance evaluation- is based on the notion of universal distribution [12] and the related algorithmic- information theory (a.k.a. Kolmogorov complexity) [9]. Using this theory, we- can deﬁne a universal distribution of tasks for a given AI realm, and sort them- according to their (objective) complexity. There are some early works which- 2' metadata={'source': 'docs\\\\test.pdf', 'page': 2}\n",
      "page_content='develop these ideas to construct intelligence tests. First, [1] suggested the in-- troduction of inductive inference problems in a somehow induction-enhanced- orcompression-enhanced Turing Test [15]. Second, [3] derived intelligence tests- (C-tests) as sets of sequence prediction problems which were generated by a uni-- versal distribution, and the result (the intelligence of the agent) was a sum of- performances for a range of problems of increasing complexity. The complexity- of each sequence was derived from its Kolmogorov complexity (a Levin variant- was used). This kind of problem (discrete sequence prediction), although typical- in IQ tests, is a narrow AI realm. In fact, [11] showed that relatively simple- algorithms could score well at IQ tests (and, as a consequence, at C-tests). In- [3] the suggestion of using interactive tasks where “rewards and penalties could- be used instead” was made. Later, Legg and Hutter (e.g. [7],[8]) gave a precise- deﬁnition to the term “Universal Intelligence”, also grounded in Kolmogorov- complexity and Solomonoﬀ’s prediction theory, as a sum (or weighted average)- of performances in all the possible RL-like environments. However, in order to- make a feasible test by extending from (static) sequences to (dynamic) envi-- ronments, several issues had to be solved ﬁrst. In [5], they address the problem- of ﬁnding a ﬁnite sample of environments and sessions, as well as appropriate- approximations to Kolmogorov complexity, the inclusion of time, and the proper- aggregation of rewards. The theory, however, has not been put into practice until- now in the form of a real test, in order to evaluate artiﬁcial and biological agents,- and, interestingly, to compare them. In this paper, we use a (simpliﬁed) imple-- mentation of this test (non-anytime) [5] using the environment class introduced- in [4] to compare Q-learning with Homo sapiens .- From this comparison we want to answer several questions. Are these tests- general enough? Does the complexity of the exercises correlate with the success- rate of Q-learning and humans? Does the diﬀerence correspond to the real dif-- ference in intelligence between these two kinds of agents? What implications do- the results have on the notion of universal intelligence and the tests that attempt- to measure it? Answering all these questions is the goal of this paper.- The choice of a proper environment class is a crucial issue in any intelligence- test. This is what [4] attempts, a hopefully unbiased environment class (called- Λ) with spaces and agents with universal descriptive (Turing-complete) power.- Basically, this environment considers a space as a graph with a diﬀerent (and- variable) topology of actions. Objects and agents can be introduced using Turing-- complete languages to generate their movements. Rewards are rational numbers- in the interval r\u00011,1 sand are generated by two special agents Good andEvil,- which leave rewards in the cells they visit. Good andEvil have the same pattern- for behaviour except for the sign of the reward ( \u0000forGood , \u0001forEvil).- The environment class Λis shown in [4] to have two relevant properties- for a performance test: (1) their environments are always balanced (a random- agent has expected reward 0), and (2) their environments are reward-sensitive- (there is no sequence of actions such that the agent can be stuck in a heaven or- hell situation, where rewards are positive or negative independently of what the- agent may do). As argued in [5], these two properties are very important for the- 3' metadata={'source': 'docs\\\\test.pdf', 'page': 3}\n",
      "page_content='environments to be discriminative and comparable (and hence the results being- properly aggregated into a single score, a performance or intelligence score). No- other properties are imposed, such as (e.g.) environments being Markov processes- or being ergodic. For more details of the environment class Λ, see [4].- 3 Test setting and administration- Following the deﬁnition of the environment class Λ, we perform some simpliﬁca-- tions to generate each environment. For instance, speed is not considered thus- being a non-anytime version of the test presented in [5]. In addition, we do not- use a Turing-complete algorithm to generate the environments. Spaces are gen-- erated by ﬁrst determining the number of cells nc, which is given by a number- between 2 and 9, using an geometric / ‘unary’ distribution (i.e. prob pn q \u00102\u0001n,- and normalising to sum up to 1). Similarly, the number of actions nais deﬁned- with a uniform distribution between 2 and nc. Both cells and actions are indexed- with natural numbers. There is a special action 0 which connects every cell with- itself (it is always possible to stay at the cell). A cell which is accessible from- another cell using one action is called a ‘neighbouring’ or adjacent cell. The con-- nections between cells are created by using a uniform distribution for each pair- of cell and action, which assigns the destination cell for each pair. We consider- the posibility that some actions may be disabled. Fig. 1 shows an example of a- randomly generated space.- c1c2c3c4c5a1a2- a1a1- a2a2a2- Fig. 1. A space with 5 cells and 3 actions ( a0, a1, a2). Reﬂexive action a0is not shown.- The number of cells and actions is, of course, related to the complexity of- the space, but not monotonically related to its Kolmogorov complexity (or a- computable variant such as Levin’s Kt). Nonetheless, most of the actual grading- of environments comes from the behaviour of Good andEvil. The sequence- of actions for Good andEvil is deﬁned by using a uniform distribution for- each element in the sequence, and a unary (geometric) distribution to determine- whether to stop the sequence, by using a probability of stopping ( pstop). An- example of sequence for the space in Fig. 1 is 201210200, which means the- execution of actions a2,a0,a1,a2, etc. Consider, e.g., that Good is placed at- cellc5. Since the pattern starts with ‘2’, Good will move (via a2) to cellc1. The- agentsGood andEvil take one action from the sequence and execute it for each- step. When the actions are exhausted, the sequence is started all over again. If- an action is not allowed at a particular cell, the agent does not move.- Initially, each agent is randomly (using a uniform distribution) placed in a- cell. Then, we let Good ,Evil and the evaluated agent interact for a certain- number of steps m. We call this an exercise (or episode). For an exercise we- average the obtained rewards, so giving a score of the agent in the environment.- 4' metadata={'source': 'docs\\\\test.pdf', 'page': 4}\n",
      "page_content='A test is a sequence of exercises or episodes. We will use 7 environments,- each with a number of cells ( nc) from 3 to 9. The size of the patterns for Good- andEvil will be made proportional (on average) to the number of cells, using- pstop \u00101 {nc. In each environment, we will allow 10 \u0002 pnc \u00011 qsteps so the agents- have the chance to detect any pattern in the environment (exploration) and also- have some further steps to exploit the ﬁndings (in case a pattern is actually- conceived). The limitation of the number of environments and steps is justiﬁed- because the tests is meant to be applied to biological agents in a reasonable- period of time (e.g., 20 minutes) and we estimate an average of 4 seconds per- action. Table 1 shows the choices we have made for the test:- Env. # No. cells ( nc)No. steps ( m)pstop- 1 3 20 1 {3- 2 4 30 1 {4- 3 5 40 1 {5- 4 6 50 1 {6- 5 7 60 1 {7- 6 8 70 1 {8- 7 9 80 1 {9- TOTAL - 350 -- Table 1. Setting for the 7 environments which compose the test.- Although [4] suggests a partially-observable interface, here we make it fully-- observable, so agents see all the cells, the actions and their contents. The agents- do not know in advance who Good is and who Evil is. They have to guess that.- 4 Agents and interfaces- 4.1 An AI agent: Q-learning- The choice of Q-learning is, of course, one of many possible choices for a reinforce-- ment learning algorithm. The reason is deliberate because we want a standard- algorithm to be evaluated ﬁrst, and, most especially, because we do not want to- evaluate (at the moment) very specialised algorithms for ergodic environments- or algorithms with better computational properties (e.g. delayed Q-learning [13]- would be a better option if speed were an issue). We use an oﬀ-the-shelf imple-- mentation of Q-learning, as explained in [18] and [14].- We use the description of cell contents as a state. We choose Q-learning’s pa-- rameters as α \u00100.05learning rate andγ \u00100.35discount factor . The parameters- have been chosen by trying 20 consecutive values for αandγbetween 0 and 1.- These 400 combinations have been evaluated for 1,000 sessions each using ran-- dom environments of diﬀerent size and complexity and episodes of 10,000 steps.- This choice is, of course, beneﬁcial for Q-learning’s performance in the tests.- Since we have rewards between -1 and 1, the elements in the Qmatrix are set- to 2.0 initially (rewards are normalised between 0 and 2 to always be positive).- 4.2 A biological agent: Homo sapiens- We took 20 humans from a University Department (PhD students, research and- teaching staﬀ) with ages ranging between 20 and 50.- 5' metadata={'source': 'docs\\\\test.pdf', 'page': 5}\n",
      "page_content='Fig. 2. A snapshot of the interface for humans. The agent has just received a positive- reward, shown with the circle with an upwards arrow. The image also shows the agent- located in cell 3, and Evil andGood are placed in cells 2 and 3 respectively. The agent- can move to cell 1 and cell 3. Cell 3 is highlighted since the mouse pointer is over it.- The interface for humans has been designed with the following principles in- mind: i) the signs used to represent observations should not have an implicit- meaning for the subject, to avoid bias in favour of humans (e.g. no skull-and-- bones for the Evil agent), ii) actions and rewards should be easily interpreted- by the subject, to avoid a cognitive overhead that would bias the experiment in- favour of Q-learning. This way, the following design decisions have been made- (Fig. 2 shows a snapshot of the interface). At the beginning of the test, the- subject is presented the task instructions, which strictly contain what the user- should know. The cells are represented by coloured squares. Agents are repre-- sented by symbols that aim to be ‘neutral’ (e.g., ♦stands forEvil and \u000fstands- forGood in the third environment, and ⃝represents the subject in every envi-- ronment). Accessible cells have a thicker border than non-accessible ones. When- the subject rolls the mouse pointer over an accessible cell, this cell is highlighted- using a double border and increasing the saturation of the background colour.- Positive, neutral and negative rewards are represented by an upwards arrow in- a green circle, a small square in a grey circle, and a downwards arrow in a red- circle, respectively. The test and its interface for humans can be downloaded- from http://users.dsic.upv.es/proy/anynt/human1/test.html .- 5 Results- We performed 20 tests (with 7 exercises each) with the setting shown in Table- 1 and we administered each of them to a human and to Q-learning.- Q−learning- RewardsDensity- −1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5- Human- RewardsDensity- −1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5- Fig. 3. Histograms of the (20 \u00027 \u0010) 140 exercises for Q-learning (left) and humans- (right). Lines show the probability densities.- 6' metadata={'source': 'docs\\\\test.pdf', 'page': 6}\n",
      "page_content='The ﬁrst observation from this paired set of results comes from the means.- While Q-learning has an overall mean of 0.259, humans show a mean of 0.237.- The standard deviations are 0.122 and 0.150 respectively. Figure 3 shows the- histograms and the probability densities (estimated by the R package).- To see the results in more detail in terms of the exercise, Figure 4 (left)- shows the results aggregating by exercise (there is one exercise for each number- of cells between 3 and 9, so totalling 7 exercises per test). This ﬁgure shows the- mean, median and dispersion of both Q-learning and humans for each exercise.- Looking at the boxplots for each space size we also see that there is no signiﬁcant- diﬀerence in terms of how Q-learning and humans perform in each of the seven- exercises. While means are around 0.2 and 0.3, variances are smaller the larger- the number of cells is. This is explained because the exercise with higher number- of cells has a higher number of iterations (see Table 1).- 3456789−1.0 −0.5 0.0 0.5 1.0- Number of CellsAverage Reward●●- ●●●●●- 3456789−1.0 −0.5 0.0 0.5 1.0- Number of CellsAverage Reward- ●QLearning- Human- ●●- 15 20 25 30−1.0 −0.5 0.0 0.5 1.0- ComplexityAverage Reward●●- ●●- ●●- ●●- ●- ●●- ●- ●- ●●●●- ●- ●- ●●- ●●- ●- ●●- ●- ●●- ●- ●●- ●●- ●- ●●- ●- ●- ●●●- ●●- ●- ●●●- ●●- ●- ●●- ●- ●●●- ●- ●●- ●●●- ●●- ●●- ●●- ●●- ●●●- ●●- ●- ●●●●●- ●●●- ●●●- ●●●●●- ● ●●- ●- ●●●- ●●●- ●●- ●- ●●- ●- ●- ●●- ●●●- ●- ●●●- ●●- ●- ●●●●- ●●●- ●●- ● ●●- ●● ●●- ●●- ●QLearning- Human- Fig. 4. Left: Box (whisker) plots for the seven exercises depending on the agent. Medi-- ans are shown in the box as a short black segment. Means are connected by a continuous- line for Q-learning and a dashed line for humans. Right: the average reward results for- the 20 \u00027 \u00022 \u0010280 exercises using Kapproxas a measure of complexity.- We applied two-way repeated measures ANOVA (agent \u0002number of cells).- ANOVA showed no statistically signiﬁcant eﬀects neither for agent ( F1,19 \u0010.461,- P \u0010.506), nor for the number of cells ( F6,114 \u0010.401,P \u0010.877). No statistically- signiﬁcant interaction eﬀect was found ( F6,114 \u0010.693,P \u0010.656) either.- Finally, since the size of the space is not a measure of complexity, we explored- the relation with the complexity of the environments. In order to approximate- this complexity, we used the size of the compressed pattern for Good andEvil,- denoted by P. More formally, given an environment µ, we calculate an approxi-- mation to its (Kolmogorov) complexity, denoted by Kapproxas follows:- Kapprox\u0010LZ pP qq- For instance, if a pattern is P=“20122220022222200222222002 ”, we compress the- string (using the memCompress function in R, with a GNU project implemen-- tation of Lempel-Ziv coding). The length of the compressed string is 19.- Figure 4 (right) shows each of the 20 \u00027 \u0010140 exercises for each kind of agent.- Again we see a higher dispersion for humans than for Q-learning (the 20 humans- 7' metadata={'source': 'docs\\\\test.pdf', 'page': 7}\n",
      "page_content='are diﬀerent, while Q-learning is exactly the same algorithm for each of the 20- tests). We calculate the Pearson correlation coeﬃcient between complexity and- reward. Now we do ﬁnd a statistically signiﬁcant correlation both for humans- (r \u0010 \u0001.257,n \u0010140,P \u0010.001) and for Q-learning ( r \u0010 \u0001.444,n \u0010140,P  - .001). We also analyse these correlations by number of cells, as shown in Table- 2. This table shows Pearson correlation coeﬃcients and associated signiﬁcance- levels (one tailed test) between “complexity” and “reward” by “numbers of cells”- for each agent. All n \u001020.- Agent 3 cells 4 cells 5 cells 6 cells 7 cells 8 cells 9 cells- Human -.474 (.017) -.134 (.286) -.367 (.056) -.515 (.010) -.282 (.114) -.189 (.213) -.146 (.270)- Q-learning -.612 (.002) -.538 (.008) -.526 (.009) -.403 (.039) -.442 (.026) -.387 (.046) -.465 (.019)- Table 2. Pearson correlation coeﬃcients and pvalues (in parentheses) between “com-- plexity” and “reward” by “numbers of cells”.- We see that correlations are stronger and always signiﬁcant for Q-learning,- while they are milder (and not always signiﬁcant) for humans. This may be- explained because humans are not reset between exercises. In general, we would- need more data (more tests) to conﬁrm or refute this hypothesis.- 6 Discussion- In section 2 we outlined several questions. One question is whether the test is- general enough. It is true that we have made many simpliﬁcations to the environ-- ment class, in such a way that Good andEvil do not react to the environment- (they just execute a cyclical sequence of actions as a pattern), and we have used- a very simple approximation to complexity instead of better approximations to- Kolmogorov complexity or Levin’s Kt. In addition, and the parameters for Q-- learning have been chosen to be optimal for these kinds of spaces and patterns.- Besides, humans are not (cannot be) reset between exercises. Despite all these- issues (most of) which are in favour of Q-learning, we think (although this can-- not be concluded in an absolute way) that the tests are not general enough.- Q-learning is not the best AI algorithm available nowadays (in fact we do not- consider Q-learning very intelligent). So, the results are not representing the real- diﬀerence in intelligence between humans and Q-learning.- A possibility is that our sample size is perhaps too small. Having more envi-- ronments of higher complexity and letting the agents interact longer with each- of them may perhaps portray a diﬀerent picture. Nonetheless, it is not clear- that humans can scale up well in this kind of exercise, especially if no part of- previous exercises can be reused to other exercises. First, some of the patterns- which appeared in the most complex exercises were considered very diﬃcult by- humans. Second, Q-learning requires many interactions to converge, so perhaps- this would only exaggerate the diﬀerence in favour of Q-learning. In any case,- this should be properly analysed with further experiments.- A more fundamental issue is whether we are testing on the wrong sort of- environments. The environment class is a general class which includes two sym-- metrical agents, Good andEvil, which are in charge of rewards. We do not think- 8' metadata={'source': 'docs\\\\test.pdf', 'page': 8}\n",
      "page_content='that this environment class is, in any case, biased against humans (the contrary- can be argued, though). In the end, the question of whether a test is biased- is diﬃcult to answer, since any single choice implies a certain bias. So, in our- opinion, the problem might be found in the environment distribution. Choosing- the universal distribution gives high probability to very simple examples with- very simple patterns, but more importantly, makes any kind of rich interaction- impossible even in environments of high Kolmogorov complexity. So, a better- environment distribution (and perhaps class) should give more probability to- incremental knowledge acquisition, social capabilities and more reactivity.- This goal towards more knowledge-intensive tasks has the risk of focussing- on knowledge and language, or to embark on Ttests without any theoretical- background, such as Jeopardy-like contests. The generality of these tasks may- be high, although the adaptability and the required learning abilities might be- low. This is something recurrent in psychometrics, where it is important (but- diﬃcult) to distinguish between knowledge acquisition capabilities and knowl-- edge application. And it is also a challenge for RL-like evaluations and systems,- where knowledge acquisition usually starts from scratch and is not incremental.- So, one of the things that we have learnt is that the change of universal- distributions from passive environments (as originally proposed in [1] and [3]) to- interactive environments (as also suggested in [3] and fully developed in [7, 8]) is- in the right direction, but it is not the solution yet. It is clear that it allows for- a more natural interpretation of the notion of intelligence as performance in a- wide range of environments, and it eases the application of tests outside humans- and machines (children, apes, etc.), but there are some other issues we have- to address to give an appropriate deﬁnition of intelligence and a practical test.- The proposal for an adaptive test [5] introduces many new ideas about creating- practical intelligence tests, and the universal distribution is substituted by an- adaptive distribution, so allowing a faster convergence to complexity levels which- are more appropriate for the agent. Nonetheless, we think that the priority is- in deﬁning new environment distributions which can give higher probability to- environments where intelligence can show its full potential (see, e.g. [6]).- Summing up, while there has been some work on comparing humans and- machines on some speciﬁc tasks, e.g., humans and Q-learning in [2], this paper- may start a series of experimental research comparing several artiﬁcial agents- (such as other algorithms in reinforcement learning, MonteCarlo AIXI [16], etc.)- and other biological agents (children, other apes, etc) for general tasks. This- might be a highly valuable source of information about whether the concept of- universal intelligence evaluation works, by trying to construct more and more- general (and universal) intelligence tests. This could lead eventually to a new- discipline, for which we already suggest a name: “universal psychometrics”.- Acknowledgments- We thank the anonymous reviewers for their helpful comments. We also thank- Jos´ e Antonio Mart´ ın H. for helping us with several issues about the RL compe-- 9' metadata={'source': 'docs\\\\test.pdf', 'page': 9}\n",
      "page_content='tition, RL-Glue and reinforcement learning in general. We are also grateful to- all the subjects who took the test. We also thank the funding from the Span-- ish MEC and MICINN for projects TIN2009-06078-E/TIN, Consolider-Ingenio- CSD2007-00022 and TIN2010-21062-C02, for MEC FPU grant AP2006-02323,- and Generalitat Valenciana for Prometeo/2008/051.- References- 1. D. L. Dowe and A. R. Hajek. A non-behavioural, computational extension to the- Turing Test. In Intl. Conf. on Computational Intelligence & multimedia applica-- tions (ICCIMA’98), Gippsland, Australia , pages 101–106, 1998.- 2. D. Gordon and D. Subramanian. A cognitive model of learning to navigate. In- Proc. 19th Conf. of the Cognitive Science Society, 1997 , volume 25, page 271.- Lawrence Erlbaum, 1997.- 3. J. Hern´ andez-Orallo. Beyond the Turing Test. J. Logic, Language & Information ,- 9(4):447–466, 2000.- 4. J. Hern´ andez-Orallo. A (hopefully) non-biased universal environment class for- measuring intelligence of biological and artiﬁcial systems. In M. Hutter et al.,- editor, Artiﬁcial General Intelligence, 3rd Intl Conf , pages 182–183. Atlantis Press,- Extended report at http://users.dsic.upv.es/proy/anynt/unbiased.pdf, 2010.- 5. J. Hern´ andez-Orallo and D. L. Dowe. Measuring universal intelligence: Towards- an anytime intelligence test. Artiﬁcial Intelligence , 174(18):1508 – 1539, 2010.- 6. J. Hern´ andez-Orallo, D.L. Dowe, S. Espa˜ na-Cubillo, M.V. Hern´ andez-Lloreda, and- J. Insa-Cabrera. On more realistic environment distributions for deﬁning, evalu-- ating and developing intelligence. In J. Schmidhuber and K.R. Th´ orisson (eds),- editors, Artiﬁcial General Intelligence 2011 . LNAI series, Springer, 2011.- 7. S. Legg and M. Hutter. A universal measure of intelligence for artiﬁcial agents. In- Intl Joint Conf on Artiﬁcial Intelligence, IJCAI , volume 19, page 1509, 2005.- 8. S. Legg and M. Hutter. Universal intelligence: A deﬁnition of machine intelligence.- Minds and Machines , 17(4):391–444, 2007.- 9. M. Li and P. Vit´ anyi. An introduction to Kolmogorov complexity and its applica-- tions (3rd ed.) . Springer-Verlag New York, Inc., 2008.- 10. G. Oppy and D. L. Dowe. The Turing Test. In Edward N.- Zalta, editor, Stanford Encyclopedia of Philosophy . Stanford University, 2011.- http://plato.stanford.edu/entries/turing-test/.- 11. P. Sanghi and D. L. Dowe. A computer program capable of passing IQ tests. In- 4th Intl. Conf. on Cognitive Science (ICCS’03), Sydney , pages 570–575, 2003.- 12. R. J. Solomonoﬀ. A formal theory of inductive inference. Part I. Information and- control , 7(1):1–22, 1964.- 13. A.L. Strehl, L. Li, E. Wiewiora, J. Langford, and M.L. Littman. PAC model-free- reinforcement learning. In Proc. of the 23rd Intl Conf on Machine learning , ICML- ’06, pages 881–888, New York, 2006.- 14. R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction . The MIT- press, 1998.- 15. A. M. Turing. Computing machinery and intelligence. Mind , 59:433–460, 1950.- 16. J. Veness, K.S. Ng, M. Hutter, and D. Silver. A Monte Carlo AIXI Approximation.- Journal of Artiﬁcial Intelligence Research, JAIR , 40:95–142, 2011.- 17. L. von Ahn, M. Blum, and J. Langford. Telling humans and computers apart- automatically. Communications of the ACM , 47(2):56–60, 2004.- 18. C.J.C.H. Watkins and P. Dayan. Q-learning. Mach. learning , 8(3):279–292, 1992.- 10- View publication stats' metadata={'source': 'docs\\\\test.pdf', 'page': 10}\n"
     ]
    }
   ],
   "source": [
    "for d in documents:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/221328895- Comparing Hu mans and AI Agents- Conf erence Paper  · August 2011- DOI: 10.1007/978-3-642-22887-2_13  · Sour ce: DBLP- CITATIONS- 50READS- 963- 5 author s, including:- Sergio Esp aña- Utrecht Univ ersity- 114 PUBLICA TIONS    1,187  CITATIONS    - SEE PROFILE- Victoria Hernánde z-Llor eda- Complut ense Univ ersity of Madrid- 34 PUBLICA TIONS    1,956  CITATIONS    - SEE PROFILE- Jose Hernande z-Orallo- Univ ersitat P olitècnic a de V alència- 255 PUBLICA TIONS    5,290  CITATIONS    - SEE PROFILE- All c ontent f ollo wing this p age was uplo aded b y Sergio Esp aña  on 02 June 2014.- The user has r equest ed enhanc ement of the do wnlo aded file.' metadata={'source': 'docs\\\\test.pdf', 'page': 0}\n",
      "page_content='Comparing humans and AI agents- Javier Insa-Cabrera1David L. Dowe2Sergio Espa˜ na-Cubillo3- M.Victoria Hern´ andez-Lloreda4Jos´ e Hern´ andez-Orallo1- 1DSIC, Universitat Polit` ecnica de Val` encia, Spain. {jinsa, jorallo}@dsic.upv.es- 2Clayton School of Information Technology, Monash University, Australia.- david.dowe@monash.edu- 3ProS Research Center, Universitat Polit` ecnica de Val` encia, Spain.- sergio.espana@pros.upv.es- 4Departamento de Metodolog´ ıa de las Ciencias del Comportamiento, Universidad- Complutense de Madrid, Spain. vhlloreda@psi.ucm.es- Abstract. Comparing humans and machines is one important source of- information about both machine and human strengths and limitations.- Most of these comparisons and competitions are performed in rather- speciﬁc tasks such as calculus, speech recognition, translation, games,- etc. The information conveyed by these experiments is limited, since it- portrays that machines are much better than humans at some domains- and worse at others. In fact, CAPTCHAs exploit this fact. However,- there have only been a few proposals of general intelligence tests in the- last two decades, and, to our knowledge, just a couple of implementations- and evaluations. In this paper, we implement one of the most recent test- proposals, devise an interface for humans and use it to compare the- intelligence of humans and Q-learning, a popular reinforcement learning- algorithm. The results are highly informative in many ways, raising many- questions on the use of a (universal) distribution of environments, on the- role of measuring knowledge acquisition, and other issues, such as speed,- duration of the test, scalability, etc.- Keywords: Intelligence measurement, universal intelligence, general vs.- speciﬁc intelligence, reinforcement learning, IQ tests.- 1 Introduction- It is well-known that IQ tests are not useful for evaluating the intelligence of- machines. The main reason is not because machines are not able to ‘understand’- the test.' metadata={'source': 'docs\\\\test.pdf', 'page': 1}\n",
      "page_content='of- machines. The main reason is not because machines are not able to ‘understand’- the test. The real reason is scarcely known and poorly understood, since available- theories do not manage to fully explain the empirical observations: it has been- shown that relative simple programs can be designed to score well on these tests- [11]. Some other approaches such as the Turing Test [15] and Captchas [17] have- their niches, but they are also inappropriate to evaluate AGI systems.- In the last ﬁfteen years, several alternatives for a general (or universal) intel-- ligence test (or deﬁnition) based on Solomonoﬀ’s universal distributions [12] (or- related ideas such as MML, compression or Kolmogorov complexity) have been' metadata={'source': 'docs\\\\test.pdf', 'page': 1}\n",
      "page_content='appearing on the scene [1, 3, 7, 8, 5], claiming that they are able to deﬁne or eval-- uate (machine) intelligence. In this paper we use one of these tests, a prototype- based on the anytime intelligence test presented in [5] and the environment class- introduced in [4], to evaluate one easily accessible biological system ( Homo sapi-- ens) and one oﬀ-the-shelf AI system, a popular reinforcement algorithm known- as Q-learning [18]. In order to do the comparison we use the same environment- class for both types of systems and we design hopefully non-biased interfaces for- both. We perform a pilot experiment on a reduced group of individuals.- From this experiment we obtain a number of interesting ﬁndings and insights.- First, it is possible to do the same test for humans and machines without being- anthropomorphic. The test is exactly the same for both and it is founded on a- theory derived from sound computational concepts. We just adapt the interface- (what way rewards, actions and observations look like) depending on the type of- subjects. Second, humans are not better than Q-learning in this test, even though- the test (despite several simpliﬁcations) is based on a universal distribution of- environments over a very general environment class. Third, since these results- are consistent to those in [11] (which show that machines can score well in IQ- tests), this gives additional evidence that a test which is valid for humans or- for machines separately might be useless to distinguish or to place humans and- machines on the same scale, so failing to be a universal intelligence test.- The following section overviews the most important proposals on deﬁning- and measuring machine intelligence to date, and, from them, it describes the- intelligence test and the environment class we will use in this paper. Sections- 3 and 4 describe the testing setting, the two types of agents we evaluate (Q-- learning and humans) and their interfaces. Section 5 includes the comparison-' metadata={'source': 'docs\\\\test.pdf', 'page': 2}\n",
      "page_content='we evaluate (Q-- learning and humans) and their interfaces. Section 5 includes the comparison- of the experimental results, analysing them by several factors. Finally, section 6- examines these results in a deeper way and draws several conclusions about the- way universal intelligence tests should and should not be.- 2 Measuring intelligence universally- Measuring machine intelligence or, more generally, performance has been virtu-- ally relegated to a philosophical or, at most, theoretical issue in AI. Given that- state-of-the-art technology in AI is still far from truly intelligent machines, it- seems that the Turing Test [15] (and its many variations [10]) and Captchas- [17] are enough for philosophical debates and practical applications respectively.- There are also tests and competitions in restricted domains, such as competi-- tions in robotics, in game playing, in machine translation and in reinforcement- learning (RL), most notably the RL competition. All of them use a somewhat- arbitrary and frequently anthropomorphic set of tasks.- An alternative, general proposal for intelligence and performance evaluation- is based on the notion of universal distribution [12] and the related algorithmic- information theory (a.k.a. Kolmogorov complexity) [9]. Using this theory, we- can deﬁne a universal distribution of tasks for a given AI realm, and sort them- according to their (objective) complexity. There are some early works which- 2' metadata={'source': 'docs\\\\test.pdf', 'page': 2}\n",
      "page_content='develop these ideas to construct intelligence tests. First, [1] suggested the in-- troduction of inductive inference problems in a somehow induction-enhanced- orcompression-enhanced Turing Test [15]. Second, [3] derived intelligence tests- (C-tests) as sets of sequence prediction problems which were generated by a uni-- versal distribution, and the result (the intelligence of the agent) was a sum of- performances for a range of problems of increasing complexity. The complexity- of each sequence was derived from its Kolmogorov complexity (a Levin variant- was used). This kind of problem (discrete sequence prediction), although typical- in IQ tests, is a narrow AI realm. In fact, [11] showed that relatively simple- algorithms could score well at IQ tests (and, as a consequence, at C-tests). In- [3] the suggestion of using interactive tasks where “rewards and penalties could- be used instead” was made. Later, Legg and Hutter (e.g. [7],[8]) gave a precise- deﬁnition to the term “Universal Intelligence”, also grounded in Kolmogorov- complexity and Solomonoﬀ’s prediction theory, as a sum (or weighted average)- of performances in all the possible RL-like environments. However, in order to- make a feasible test by extending from (static) sequences to (dynamic) envi-- ronments, several issues had to be solved ﬁrst. In [5], they address the problem- of ﬁnding a ﬁnite sample of environments and sessions, as well as appropriate- approximations to Kolmogorov complexity, the inclusion of time, and the proper- aggregation of rewards. The theory, however, has not been put into practice until- now in the form of a real test, in order to evaluate artiﬁcial and biological agents,- and, interestingly, to compare them. In this paper, we use a (simpliﬁed) imple-- mentation of this test (non-anytime) [5] using the environment class introduced- in [4] to compare Q-learning with Homo sapiens .- From this comparison we want to answer several questions. Are these tests- general enough? Does' metadata={'source': 'docs\\\\test.pdf', 'page': 3}\n",
      "page_content='.- From this comparison we want to answer several questions. Are these tests- general enough? Does the complexity of the exercises correlate with the success- rate of Q-learning and humans? Does the diﬀerence correspond to the real dif-- ference in intelligence between these two kinds of agents? What implications do- the results have on the notion of universal intelligence and the tests that attempt- to measure it? Answering all these questions is the goal of this paper.- The choice of a proper environment class is a crucial issue in any intelligence- test. This is what [4] attempts, a hopefully unbiased environment class (called- Λ) with spaces and agents with universal descriptive (Turing-complete) power.- Basically, this environment considers a space as a graph with a diﬀerent (and- variable) topology of actions. Objects and agents can be introduced using Turing-- complete languages to generate their movements. Rewards are rational numbers- in the interval r\u00011,1 sand are generated by two special agents Good andEvil,- which leave rewards in the cells they visit. Good andEvil have the same pattern- for behaviour except for the sign of the reward ( \u0000forGood , \u0001forEvil).- The environment class Λis shown in [4] to have two relevant properties- for a performance test: (1) their environments are always balanced (a random- agent has expected reward 0), and (2) their environments are reward-sensitive- (there is no sequence of actions such that the agent can be stuck in a heaven or- hell situation, where rewards are positive or negative independently of what the- agent may do). As argued in [5], these two properties are very important for the- 3' metadata={'source': 'docs\\\\test.pdf', 'page': 3}\n",
      "page_content='environments to be discriminative and comparable (and hence the results being- properly aggregated into a single score, a performance or intelligence score). No- other properties are imposed, such as (e.g.) environments being Markov processes- or being ergodic. For more details of the environment class Λ, see [4].- 3 Test setting and administration- Following the deﬁnition of the environment class Λ, we perform some simpliﬁca-- tions to generate each environment. For instance, speed is not considered thus- being a non-anytime version of the test presented in [5]. In addition, we do not- use a Turing-complete algorithm to generate the environments. Spaces are gen-- erated by ﬁrst determining the number of cells nc, which is given by a number- between 2 and 9, using an geometric / ‘unary’ distribution (i.e. prob pn q \u00102\u0001n,- and normalising to sum up to 1). Similarly, the number of actions nais deﬁned- with a uniform distribution between 2 and nc. Both cells and actions are indexed- with natural numbers. There is a special action 0 which connects every cell with- itself (it is always possible to stay at the cell). A cell which is accessible from- another cell using one action is called a ‘neighbouring’ or adjacent cell. The con-- nections between cells are created by using a uniform distribution for each pair- of cell and action, which assigns the destination cell for each pair. We consider- the posibility that some actions may be disabled. Fig. 1 shows an example of a- randomly generated space.- c1c2c3c4c5a1a2- a1a1- a2a2a2- Fig. 1. A space with 5 cells and 3 actions ( a0, a1, a2). Reﬂexive action a0is not shown.- The number of cells and actions is, of course, related to the complexity of- the space, but not monotonically related to its Kolmogorov complexity (or a- computable variant such as Levin’s Kt). Nonetheless, most of the actual grading- of environments comes from the behaviour of Good andEvil. The sequence- of actions for Good andEvil is deﬁned by using a' metadata={'source': 'docs\\\\test.pdf', 'page': 4}\n",
      "page_content='from the behaviour of Good andEvil. The sequence- of actions for Good andEvil is deﬁned by using a uniform distribution for- each element in the sequence, and a unary (geometric) distribution to determine- whether to stop the sequence, by using a probability of stopping ( pstop). An- example of sequence for the space in Fig. 1 is 201210200, which means the- execution of actions a2,a0,a1,a2, etc. Consider, e.g., that Good is placed at- cellc5. Since the pattern starts with ‘2’, Good will move (via a2) to cellc1. The- agentsGood andEvil take one action from the sequence and execute it for each- step. When the actions are exhausted, the sequence is started all over again. If- an action is not allowed at a particular cell, the agent does not move.- Initially, each agent is randomly (using a uniform distribution) placed in a- cell. Then, we let Good ,Evil and the evaluated agent interact for a certain- number of steps m. We call this an exercise (or episode). For an exercise we- average the obtained rewards, so giving a score of the agent in the environment.- 4' metadata={'source': 'docs\\\\test.pdf', 'page': 4}\n",
      "page_content='A test is a sequence of exercises or episodes. We will use 7 environments,- each with a number of cells ( nc) from 3 to 9. The size of the patterns for Good- andEvil will be made proportional (on average) to the number of cells, using- pstop \u00101 {nc. In each environment, we will allow 10 \u0002 pnc \u00011 qsteps so the agents- have the chance to detect any pattern in the environment (exploration) and also- have some further steps to exploit the ﬁndings (in case a pattern is actually- conceived). The limitation of the number of environments and steps is justiﬁed- because the tests is meant to be applied to biological agents in a reasonable- period of time (e.g., 20 minutes) and we estimate an average of 4 seconds per- action. Table 1 shows the choices we have made for the test:- Env. # No. cells ( nc)No. steps ( m)pstop- 1 3 20 1 {3- 2 4 30 1 {4- 3 5 40 1 {5- 4 6 50 1 {6- 5 7 60 1 {7- 6 8 70 1 {8- 7 9 80 1 {9- TOTAL - 350 -- Table 1. Setting for the 7 environments which compose the test.- Although [4] suggests a partially-observable interface, here we make it fully-- observable, so agents see all the cells, the actions and their contents. The agents- do not know in advance who Good is and who Evil is. They have to guess that.- 4 Agents and interfaces- 4.1 An AI agent: Q-learning- The choice of Q-learning is, of course, one of many possible choices for a reinforce-- ment learning algorithm. The reason is deliberate because we want a standard- algorithm to be evaluated ﬁrst, and, most especially, because we do not want to- evaluate (at the moment) very specialised algorithms for ergodic environments- or algorithms with better computational properties (e.g. delayed Q-learning [13]- would be a better option if speed were an issue). We use an oﬀ-the-shelf imple-- mentation of Q-learning, as explained in [18] and [14].- We use the description of cell contents as a state. We choose Q-learning’s pa-- rameters as α \u00100.05learning rate andγ \u00100.35discount factor . The parameters- have' metadata={'source': 'docs\\\\test.pdf', 'page': 5}\n",
      "page_content='Q-learning’s pa-- rameters as α \u00100.05learning rate andγ \u00100.35discount factor . The parameters- have been chosen by trying 20 consecutive values for αandγbetween 0 and 1.- These 400 combinations have been evaluated for 1,000 sessions each using ran-- dom environments of diﬀerent size and complexity and episodes of 10,000 steps.- This choice is, of course, beneﬁcial for Q-learning’s performance in the tests.- Since we have rewards between -1 and 1, the elements in the Qmatrix are set- to 2.0 initially (rewards are normalised between 0 and 2 to always be positive).- 4.2 A biological agent: Homo sapiens- We took 20 humans from a University Department (PhD students, research and- teaching staﬀ) with ages ranging between 20 and 50.- 5' metadata={'source': 'docs\\\\test.pdf', 'page': 5}\n",
      "page_content='Fig. 2. A snapshot of the interface for humans. The agent has just received a positive- reward, shown with the circle with an upwards arrow. The image also shows the agent- located in cell 3, and Evil andGood are placed in cells 2 and 3 respectively. The agent- can move to cell 1 and cell 3. Cell 3 is highlighted since the mouse pointer is over it.- The interface for humans has been designed with the following principles in- mind: i) the signs used to represent observations should not have an implicit- meaning for the subject, to avoid bias in favour of humans (e.g. no skull-and-- bones for the Evil agent), ii) actions and rewards should be easily interpreted- by the subject, to avoid a cognitive overhead that would bias the experiment in- favour of Q-learning. This way, the following design decisions have been made- (Fig. 2 shows a snapshot of the interface). At the beginning of the test, the- subject is presented the task instructions, which strictly contain what the user- should know. The cells are represented by coloured squares. Agents are repre-- sented by symbols that aim to be ‘neutral’ (e.g., ♦stands forEvil and \u000fstands- forGood in the third environment, and ⃝represents the subject in every envi-- ronment). Accessible cells have a thicker border than non-accessible ones. When- the subject rolls the mouse pointer over an accessible cell, this cell is highlighted- using a double border and increasing the saturation of the background colour.- Positive, neutral and negative rewards are represented by an upwards arrow in- a green circle, a small square in a grey circle, and a downwards arrow in a red- circle, respectively. The test and its interface for humans can be downloaded- from http://users.dsic.upv.es/proy/anynt/human1/test.html .- 5 Results- We performed 20 tests (with 7 exercises each) with the setting shown in Table- 1 and we administered each of them to a human and to Q-learning.- Q−learning- RewardsDensity- −1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5-' metadata={'source': 'docs\\\\test.pdf', 'page': 6}\n",
      "page_content='to a human and to Q-learning.- Q−learning- RewardsDensity- −1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5- Human- RewardsDensity- −1.0 −0.5 0.0 0.5 1.00.0 0.5 1.0 1.5- Fig. 3. Histograms of the (20 \u00027 \u0010) 140 exercises for Q-learning (left) and humans- (right). Lines show the probability densities.- 6' metadata={'source': 'docs\\\\test.pdf', 'page': 6}\n",
      "page_content='The ﬁrst observation from this paired set of results comes from the means.- While Q-learning has an overall mean of 0.259, humans show a mean of 0.237.- The standard deviations are 0.122 and 0.150 respectively. Figure 3 shows the- histograms and the probability densities (estimated by the R package).- To see the results in more detail in terms of the exercise, Figure 4 (left)- shows the results aggregating by exercise (there is one exercise for each number- of cells between 3 and 9, so totalling 7 exercises per test). This ﬁgure shows the- mean, median and dispersion of both Q-learning and humans for each exercise.- Looking at the boxplots for each space size we also see that there is no signiﬁcant- diﬀerence in terms of how Q-learning and humans perform in each of the seven- exercises. While means are around 0.2 and 0.3, variances are smaller the larger- the number of cells is. This is explained because the exercise with higher number- of cells has a higher number of iterations (see Table 1).- 3456789−1.0 −0.5 0.0 0.5 1.0- Number of CellsAverage Reward●●- ●●●●●- 3456789−1.0 −0.5 0.0 0.5 1.0- Number of CellsAverage Reward- ●QLearning- Human- ●●- 15 20 25 30−1.0 −0.5 0.0 0.5 1.0- ComplexityAverage Reward●●- ●●- ●●- ●●- ●- ●●- ●- ●- ●●●●- ●- ●- ●●- ●●- ●- ●●- ●- ●●- ●- ●●- ●●- ●- ●●- ●- ●- ●●●- ●●- ●- ●●●- ●●- ●- ●●- ●- ●●●- ●- ●●- ●●●- ●●- ●●- ●●- ●●- ●●●- ●●- ●- ●●●●●- ●●●- ●●●- ●●●●●- ● ●●- ●- ●●●- ●●●- ●●- ●- ●●- ●- ●- ●●- ●●●- ●- ●●●- ●●- ●- ●●●●- ●●●- ●●- ● ●●- ●● ●●- ●●- ●QLearning- Human- Fig. 4. Left: Box (whisker) plots for the seven exercises depending on the agent. Medi-- ans are shown in the box as a short black segment. Means are connected by a continuous- line for Q-learning and a dashed line for humans. Right: the average reward results for- the 20 \u00027 \u00022 \u0010280 exercises using Kapproxas a measure of complexity.- We applied two-way repeated measures ANOVA (agent \u0002number of cells).- ANOVA showed no statistically signiﬁcant eﬀects neither for agent ( F1,19' metadata={'source': 'docs\\\\test.pdf', 'page': 7}\n",
      "page_content='\u0002number of cells).- ANOVA showed no statistically signiﬁcant eﬀects neither for agent ( F1,19 \u0010.461,- P \u0010.506), nor for the number of cells ( F6,114 \u0010.401,P \u0010.877). No statistically- signiﬁcant interaction eﬀect was found ( F6,114 \u0010.693,P \u0010.656) either.- Finally, since the size of the space is not a measure of complexity, we explored- the relation with the complexity of the environments. In order to approximate- this complexity, we used the size of the compressed pattern for Good andEvil,- denoted by P. More formally, given an environment µ, we calculate an approxi-- mation to its (Kolmogorov) complexity, denoted by Kapproxas follows:- Kapprox\u0010LZ pP qq- For instance, if a pattern is P=“20122220022222200222222002 ”, we compress the- string (using the memCompress function in R, with a GNU project implemen-- tation of Lempel-Ziv coding). The length of the compressed string is 19.- Figure 4 (right) shows each of the 20 \u00027 \u0010140 exercises for each kind of agent.- Again we see a higher dispersion for humans than for Q-learning (the 20 humans- 7' metadata={'source': 'docs\\\\test.pdf', 'page': 7}\n",
      "page_content='are diﬀerent, while Q-learning is exactly the same algorithm for each of the 20- tests). We calculate the Pearson correlation coeﬃcient between complexity and- reward. Now we do ﬁnd a statistically signiﬁcant correlation both for humans- (r \u0010 \u0001.257,n \u0010140,P \u0010.001) and for Q-learning ( r \u0010 \u0001.444,n \u0010140,P  - .001). We also analyse these correlations by number of cells, as shown in Table- 2. This table shows Pearson correlation coeﬃcients and associated signiﬁcance- levels (one tailed test) between “complexity” and “reward” by “numbers of cells”- for each agent. All n \u001020.- Agent 3 cells 4 cells 5 cells 6 cells 7 cells 8 cells 9 cells- Human -.474 (.017) -.134 (.286) -.367 (.056) -.515 (.010) -.282 (.114) -.189 (.213) -.146 (.270)- Q-learning -.612 (.002) -.538 (.008) -.526 (.009) -.403 (.039) -.442 (.026) -.387 (.046) -.465 (.019)- Table 2. Pearson correlation coeﬃcients and pvalues (in parentheses) between “com-- plexity” and “reward” by “numbers of cells”.- We see that correlations are stronger and always signiﬁcant for Q-learning,- while they are milder (and not always signiﬁcant) for humans. This may be- explained because humans are not reset between exercises. In general, we would- need more data (more tests) to conﬁrm or refute this hypothesis.- 6 Discussion- In section 2 we outlined several questions. One question is whether the test is- general enough. It is true that we have made many simpliﬁcations to the environ-- ment class, in such a way that Good andEvil do not react to the environment- (they just execute a cyclical sequence of actions as a pattern), and we have used- a very simple approximation to complexity instead of better approximations to- Kolmogorov complexity or Levin’s Kt. In addition, and the parameters for Q-- learning have been chosen to be optimal for these kinds of spaces and patterns.- Besides, humans are not (cannot be) reset between exercises. Despite all these- issues (most of) which are in favour of Q-learning, we think (although this' metadata={'source': 'docs\\\\test.pdf', 'page': 8}\n",
      "page_content='Despite all these- issues (most of) which are in favour of Q-learning, we think (although this can-- not be concluded in an absolute way) that the tests are not general enough.- Q-learning is not the best AI algorithm available nowadays (in fact we do not- consider Q-learning very intelligent). So, the results are not representing the real- diﬀerence in intelligence between humans and Q-learning.- A possibility is that our sample size is perhaps too small. Having more envi-- ronments of higher complexity and letting the agents interact longer with each- of them may perhaps portray a diﬀerent picture. Nonetheless, it is not clear- that humans can scale up well in this kind of exercise, especially if no part of- previous exercises can be reused to other exercises. First, some of the patterns- which appeared in the most complex exercises were considered very diﬃcult by- humans. Second, Q-learning requires many interactions to converge, so perhaps- this would only exaggerate the diﬀerence in favour of Q-learning. In any case,- this should be properly analysed with further experiments.- A more fundamental issue is whether we are testing on the wrong sort of- environments. The environment class is a general class which includes two sym-- metrical agents, Good andEvil, which are in charge of rewards. We do not think- 8' metadata={'source': 'docs\\\\test.pdf', 'page': 8}\n",
      "page_content='that this environment class is, in any case, biased against humans (the contrary- can be argued, though). In the end, the question of whether a test is biased- is diﬃcult to answer, since any single choice implies a certain bias. So, in our- opinion, the problem might be found in the environment distribution. Choosing- the universal distribution gives high probability to very simple examples with- very simple patterns, but more importantly, makes any kind of rich interaction- impossible even in environments of high Kolmogorov complexity. So, a better- environment distribution (and perhaps class) should give more probability to- incremental knowledge acquisition, social capabilities and more reactivity.- This goal towards more knowledge-intensive tasks has the risk of focussing- on knowledge and language, or to embark on Ttests without any theoretical- background, such as Jeopardy-like contests. The generality of these tasks may- be high, although the adaptability and the required learning abilities might be- low. This is something recurrent in psychometrics, where it is important (but- diﬃcult) to distinguish between knowledge acquisition capabilities and knowl-- edge application. And it is also a challenge for RL-like evaluations and systems,- where knowledge acquisition usually starts from scratch and is not incremental.- So, one of the things that we have learnt is that the change of universal- distributions from passive environments (as originally proposed in [1] and [3]) to- interactive environments (as also suggested in [3] and fully developed in [7, 8]) is- in the right direction, but it is not the solution yet. It is clear that it allows for- a more natural interpretation of the notion of intelligence as performance in a- wide range of environments, and it eases the application of tests outside humans- and machines (children, apes, etc.), but there are some other issues we have- to address to give an appropriate deﬁnition of intelligence and a practical' metadata={'source': 'docs\\\\test.pdf', 'page': 9}\n",
      "page_content='other issues we have- to address to give an appropriate deﬁnition of intelligence and a practical test.- The proposal for an adaptive test [5] introduces many new ideas about creating- practical intelligence tests, and the universal distribution is substituted by an- adaptive distribution, so allowing a faster convergence to complexity levels which- are more appropriate for the agent. Nonetheless, we think that the priority is- in deﬁning new environment distributions which can give higher probability to- environments where intelligence can show its full potential (see, e.g. [6]).- Summing up, while there has been some work on comparing humans and- machines on some speciﬁc tasks, e.g., humans and Q-learning in [2], this paper- may start a series of experimental research comparing several artiﬁcial agents- (such as other algorithms in reinforcement learning, MonteCarlo AIXI [16], etc.)- and other biological agents (children, other apes, etc) for general tasks. This- might be a highly valuable source of information about whether the concept of- universal intelligence evaluation works, by trying to construct more and more- general (and universal) intelligence tests. This could lead eventually to a new- discipline, for which we already suggest a name: “universal psychometrics”.- Acknowledgments- We thank the anonymous reviewers for their helpful comments. We also thank- Jos´ e Antonio Mart´ ın H. for helping us with several issues about the RL compe-- 9' metadata={'source': 'docs\\\\test.pdf', 'page': 9}\n",
      "page_content='tition, RL-Glue and reinforcement learning in general. We are also grateful to- all the subjects who took the test. We also thank the funding from the Span-- ish MEC and MICINN for projects TIN2009-06078-E/TIN, Consolider-Ingenio- CSD2007-00022 and TIN2010-21062-C02, for MEC FPU grant AP2006-02323,- and Generalitat Valenciana for Prometeo/2008/051.- References- 1. D. L. Dowe and A. R. Hajek. A non-behavioural, computational extension to the- Turing Test. In Intl. Conf. on Computational Intelligence & multimedia applica-- tions (ICCIMA’98), Gippsland, Australia , pages 101–106, 1998.- 2. D. Gordon and D. Subramanian. A cognitive model of learning to navigate. In- Proc. 19th Conf. of the Cognitive Science Society, 1997 , volume 25, page 271.- Lawrence Erlbaum, 1997.- 3. J. Hern´ andez-Orallo. Beyond the Turing Test. J. Logic, Language & Information ,- 9(4):447–466, 2000.- 4. J. Hern´ andez-Orallo. A (hopefully) non-biased universal environment class for- measuring intelligence of biological and artiﬁcial systems. In M. Hutter et al.,- editor, Artiﬁcial General Intelligence, 3rd Intl Conf , pages 182–183. Atlantis Press,- Extended report at http://users.dsic.upv.es/proy/anynt/unbiased.pdf, 2010.- 5. J. Hern´ andez-Orallo and D. L. Dowe. Measuring universal intelligence: Towards- an anytime intelligence test. Artiﬁcial Intelligence , 174(18):1508 – 1539, 2010.- 6. J. Hern´ andez-Orallo, D.L. Dowe, S. Espa˜ na-Cubillo, M.V. Hern´ andez-Lloreda, and- J. Insa-Cabrera. On more realistic environment distributions for deﬁning, evalu-- ating and developing intelligence. In J. Schmidhuber and K.R. Th´ orisson (eds),- editors, Artiﬁcial General Intelligence 2011 . LNAI series, Springer, 2011.- 7. S. Legg and M. Hutter. A universal measure of intelligence for artiﬁcial agents. In- Intl Joint Conf on Artiﬁcial Intelligence, IJCAI , volume 19, page 1509, 2005.- 8. S. Legg and M. Hutter. Universal intelligence: A deﬁnition of machine intelligence.- Minds and Machines ,' metadata={'source': 'docs\\\\test.pdf', 'page': 10}\n",
      "page_content='and M. Hutter. Universal intelligence: A deﬁnition of machine intelligence.- Minds and Machines , 17(4):391–444, 2007.- 9. M. Li and P. Vit´ anyi. An introduction to Kolmogorov complexity and its applica-- tions (3rd ed.) . Springer-Verlag New York, Inc., 2008.- 10. G. Oppy and D. L. Dowe. The Turing Test. In Edward N.- Zalta, editor, Stanford Encyclopedia of Philosophy . Stanford University, 2011.- http://plato.stanford.edu/entries/turing-test/.- 11. P. Sanghi and D. L. Dowe. A computer program capable of passing IQ tests. In- 4th Intl. Conf. on Cognitive Science (ICCS’03), Sydney , pages 570–575, 2003.- 12. R. J. Solomonoﬀ. A formal theory of inductive inference. Part I. Information and- control , 7(1):1–22, 1964.- 13. A.L. Strehl, L. Li, E. Wiewiora, J. Langford, and M.L. Littman. PAC model-free- reinforcement learning. In Proc. of the 23rd Intl Conf on Machine learning , ICML- ’06, pages 881–888, New York, 2006.- 14. R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction . The MIT- press, 1998.- 15. A. M. Turing. Computing machinery and intelligence. Mind , 59:433–460, 1950.- 16. J. Veness, K.S. Ng, M. Hutter, and D. Silver. A Monte Carlo AIXI Approximation.- Journal of Artiﬁcial Intelligence Research, JAIR , 40:95–142, 2011.- 17. L. von Ahn, M. Blum, and J. Langford. Telling humans and computers apart- automatically. Communications of the ACM , 47(2):56–60, 2004.- 18. C.J.C.H. Watkins and P. Dayan. Q-learning. Mach. learning , 8(3):279–292, 1992.- 10- View publication stats' metadata={'source': 'docs\\\\test.pdf', 'page': 10}\n"
     ]
    }
   ],
   "source": [
    "for t in docs:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
